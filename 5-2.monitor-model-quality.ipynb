{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "327dd0be-8ba8-43e5-b178-eb1db8d3d40a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <B> # SageMaker monitor for data and model quality </B>\n",
    "* Container: codna_python3\n",
    "    - Model Quaility\n",
    "        - https://sagemaker-examples.readthedocs.io/en/latest/sagemaker_model_monitor/model_quality/model_quality_churn_sdk.html\n",
    "        - https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker_model_monitor/model_quality/model_quality_churn_sdk.ipynb\n",
    "    - Data Quaility\n",
    "        - https://github.com/aws/amazon-sagemaker-examples/tree/main/sagemaker_model_monitor/introduction\n",
    "        - https://github.com/aws-samples/amazon-sagemaker-data-quality-monitor-custom-preprocessing\n",
    "        - https://sagemaker-examples.readthedocs.io/en/latest/sagemaker_model_monitor/introduction/SageMaker-ModelMonitoring.html#Create-a-baselining-job-with-training-dataset\n",
    "    - 컬럼수 안맞을때\n",
    "        - https://repost.aws/questions/QU8Xkelo1ARA2zcn4rHuk09w/questions/QU8Xkelo1ARA2zcn4rHuk09w/sagemaker-model-monitor-missing-columns-constraint-violation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ecd494-5ac0-4f66-a3cc-340e847e3c46",
   "metadata": {},
   "source": [
    "## AutoReload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c07a9ee3-c178-41b6-a212-2a33f65dc377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044d087e-68b7-42f2-89a7-4f0a84345ed4",
   "metadata": {},
   "source": [
    "## 0. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbbc26ef-076a-4962-a893-e7db013534cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "install_needed = False  # should only be True once\n",
    "# install_needed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49be1d23-7472-47f2-889e-48b92f48a183",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already revised\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "\n",
    "DAEMON_PATH=\"/etc/docker\"\n",
    "MEMORY_SIZE=10G\n",
    "\n",
    "FLAG=$(cat $DAEMON_PATH/daemon.json | jq 'has(\"data-root\")')\n",
    "# echo $FLAG\n",
    "\n",
    "if [ \"$FLAG\" == true ]; then\n",
    "    echo \"Already revised\"\n",
    "else\n",
    "    echo \"Add data-root and default-shm-size=$MEMORY_SIZE\"\n",
    "    sudo cp $DAEMON_PATH/daemon.json $DAEMON_PATH/daemon.json.bak\n",
    "    sudo cat $DAEMON_PATH/daemon.json.bak | jq '. += {\"data-root\":\"/home/ec2-user/SageMaker/.container/docker\",\"default-shm-size\":\"'$MEMORY_SIZE'\"}' | sudo tee $DAEMON_PATH/daemon.json > /dev/null\n",
    "    sudo service docker restart\n",
    "    echo \"Docker Restart\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe9fe5bf-a8f8-4275-953f-0470438c4e80",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "installing deps and restarting kernel\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (23.0.1)\n",
      "Collecting pip\n",
      "  Using cached pip-23.1-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.0.1\n",
      "    Uninstalling pip-23.0.1:\n",
      "      Successfully uninstalled pip-23.0.1\n",
      "Successfully installed pip-23.1\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: smdebug in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.0.12)\n",
      "Requirement already satisfied: sagemaker-experiments in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.1.43)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from smdebug) (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from smdebug) (1.22.3)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from smdebug) (21.3)\n",
      "Requirement already satisfied: boto3>=1.10.32 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from smdebug) (1.26.71)\n",
      "Requirement already satisfied: pyinstrument==3.4.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from smdebug) (3.4.2)\n",
      "Requirement already satisfied: pyinstrument-cext>=0.2.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pyinstrument==3.4.2->smdebug) (0.2.4)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.71 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3>=1.10.32->smdebug) (1.29.71)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3>=1.10.32->smdebug) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3>=1.10.32->smdebug) (0.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging->smdebug) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.71->boto3>=1.10.32->smdebug) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.71->boto3>=1.10.32->smdebug) (1.26.8)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.71->boto3>=1.10.32->smdebug) (1.16.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.146.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.147.0.tar.gz (718 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.7/718.7 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs<23,>=20.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (22.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.28 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.26.71)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.22.3)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (3.20.3)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.13.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.5.2)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML==5.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (5.4.1)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (3.2.0)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.6.2)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.71 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.29.71)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.0.9)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.19.3)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker) (65.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker) (2022.7)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: dill>=0.3.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.71->boto3<2.0,>=1.26.28->sagemaker) (1.26.8)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.147.0-py2.py3-none-any.whl size=965137 sha256=67f4809e70249bcb69cd12d6cceeb8d28ecc938a14f4a495c847397c35c2f880\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/a7/82/1f/1479d5a2d69b3429533baa517af5a66041c69cb3e2ca5d1041\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.146.0\n",
      "    Uninstalling sagemaker-2.146.0:\n",
      "      Successfully uninstalled sagemaker-2.146.0\n",
      "Successfully installed sagemaker-2.147.0\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: xgboost==1.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.3.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from xgboost==1.3.1) (1.22.3)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from xgboost==1.3.1) (1.10.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import IPython\n",
    "\n",
    "if install_needed:\n",
    "    print(\"installing deps and restarting kernel\")\n",
    "    !{sys.executable} -m pip install -U pip\n",
    "    !{sys.executable} -m pip install -U smdebug sagemaker-experiments\n",
    "    !{sys.executable} -m pip install -U sagemaker\n",
    "    !{sys.executable} -m pip install -U xgboost==1.3.1\n",
    "\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccc9dda-9dad-4f81-8dd5-53b5623f1d93",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. parameter store 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d6e737a-557b-44c7-a5c2-f6798bf0c430",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from utils.ssm import parameter_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f9894a8-4249-4fcc-9b44-d5f43f4fd867",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strRegionName=boto3.Session().region_name\n",
    "pm = parameter_store(strRegionName)\n",
    "strPrefix = pm.get_params(key=\"PREFIX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fe3068d-8afa-4cdd-a13e-ba2db61ed822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strBucketName = pm.get_params(key=\"-\".join([strPrefix, \"BUCKET\"]))\n",
    "strExecutionRole = pm.get_params(key=\"-\".join([strPrefix, \"SAGEMAKER-ROLE-ARN\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e91f79c-3283-418c-96b4-79559e557dfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strBucketName: sagemaker-us-east-1-419974056037\n",
      "strExecutionRole: arn:aws:iam::419974056037:role/service-role/AmazonSageMaker-ExecutionRole-20221206T163436\n"
     ]
    }
   ],
   "source": [
    "print (f'strBucketName: {strBucketName}')\n",
    "print (f'strExecutionRole: {strExecutionRole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968b6452-fb2f-4ed9-aae9-55e310d567ba",
   "metadata": {},
   "source": [
    "## 2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e227473a-2ddc-48aa-bc76-4ba310454749",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaecc942-d54d-44d0-8552-ec183e833bf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strS3DataPath = f\"s3://{strBucketName}/dataset\" \n",
    "strLocalDataPath = os.path.join(os.getcwd(), \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0674243-6019-4e5e-8563-2fd17ca8ae95",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.Depoly with Data capture\n",
    "- https://github.com/aws-samples/amazon-sagemaker-data-quality-monitor-custom-preprocessing\n",
    "- https://github.com/aws/amazon-sagemaker-examples/tree/main/sagemaker_model_monitor/introduction\n",
    "- https://sagemaker-examples.readthedocs.io/en/latest/sagemaker_model_monitor/introduction/SageMaker-ModelMonitoring.html#Create-a-baselining-job-with-training-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5b083e-f425-4b60-ad0a-77bc78badc06",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1 Check functions in local mode\n",
    "[중요] inference.py를 만들어 주어야 함\n",
    "* model_fn: 학습한 모델 로드\n",
    "* input_fn: endpoint invocation시 전달 되는 input 처리 하는 함수\n",
    "* predict_fn: forword propagation, input_fn의 이후 호출 \n",
    "* output_fn: 유저에게 결과 전달\n",
    "\n",
    "- 사용자 정의 inference 코드를 정의해서 사용하기 전에, 노트북에서 사전 테스트 및 디버깅을 하고 진행하면 빠르게 추론 개발을 할수 있습니다.\n",
    "- 디폴트 inference code (input_fn, model_fn, predict_fn, output_fn) 을 사용해도 되지만, 상황에 따라서는 사용자 정의가 필요할 수 있습니다. 디폴트 코드는 아래 링크를 참고 하세요.\n",
    "    - [Deploy PyTorch Models](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#deploy-pytorch-models)\n",
    "    - [디폴트 inference Code](https://github.com/aws/sagemaker-pytorch-inference-toolkit/blob/master/src/sagemaker_pytorch_serving_container/default_pytorch_inference_handler.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19b8cd5-0afd-4459-b5ae-8b405de51e0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 로컬 모드 수행시, 새로운 로컬모드 수행을 위해서는 이전 사용했던 도커는 반드시 stop 해줘야 한다\n",
    "* docker ps -a 로 현재 수행중인 contatiner ID 확인 후\n",
    "* docker stop \"<<contatiner ID>>\"\n",
    "* docker container prune -f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5503d61a-819c-4eee-bc00-cd19f610460c",
   "metadata": {},
   "source": [
    "* 3.1.1 inference.py 생성\n",
    "    - https://aws.amazon.com/ko/blogs/machine-learning/design-a-compelling-record-filtering-method-with-amazon-sagemaker-model-monitor/\n",
    "    -  We also need to ensure that Flask Response is returned to match both input and output content types exactly. It is a necessary step for Model Monitor to work for the image running Gunicorn/Flask. The content type of output data captured by Model Monitor, which only works with CSV or JSON, is Base64 by default unless Response() explicitly converts it to a specific type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33ddec06-0ce2-4126-ba5f-319cf2f93f8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting source/deploy/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile source/deploy/inference.py\n",
    "import io\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "import xgboost as xgb\n",
    "import sagemaker_xgboost_container.encoder as xgb_encoders\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from io import StringIO\n",
    "\n",
    "#For Gunicorn/Flask xgboost image, we need to ensure input and output encoding match exactly for model monitor (CSV or JSON)\n",
    "from flask import Response \n",
    "\n",
    "NUM_FEATURES = 58\n",
    "CSV_SERIALIZER = CSVSerializer(content_type='text/csv')\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Deserialize and return fitted model.\n",
    "    \"\"\"\n",
    "    model_file = \"xgboost-model\"\n",
    "    model = xgb.Booster()\n",
    "    model.load_model(os.path.join(model_dir, model_file))\n",
    "    return model\n",
    "                     \n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"\n",
    "    The SageMaker XGBoost model server receives the request data body and the content type,\n",
    "    and invokes the `input_fn`.\n",
    "    Return a DMatrix (an object that can be passed to predict_fn).\n",
    "    \"\"\"\n",
    "\n",
    "    print (f'Input, Content_type: {request_content_type}')\n",
    "    if request_content_type == \"application/x-npy\":        \n",
    "        stream = BytesIO(request_body)\n",
    "        array = np.frombuffer(stream.getvalue())\n",
    "        array = array.reshape(int(len(array)/NUM_FEATURES), NUM_FEATURES)\n",
    "        return xgb.DMatrix(array)\n",
    "    \n",
    "    elif request_content_type == \"text/csv\":\n",
    "        return xgb_encoders.csv_to_dmatrix(request_body.rstrip(\"\\n\"))\n",
    "    \n",
    "    elif request_content_type == \"text/libsvm\":\n",
    "        return xgb_encoders.libsvm_to_dmatrix(request_body)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Content type {} is not supported.\".format(request_content_type)\n",
    "        )\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"\n",
    "    SageMaker XGBoost model server invokes `predict_fn` on the return value of `input_fn`.\n",
    "\n",
    "    Return a two-dimensional NumPy array (predictions and scores)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    y_probs = model.predict(input_data)\n",
    "    print(\"--- Inference time: %s secs ---\" % (time.time() - start_time))    \n",
    "    y_preds = [1 if e >= 0.5 else 0 for e in y_probs] \n",
    "    #return np.vstack((y_preds, y_probs))\n",
    "    y_probs = np.array(y_probs).reshape(1, -1)\n",
    "    y_preds = np.array(y_preds).reshape(1, -1)   \n",
    "    output = np.concatenate([y_probs, y_preds], axis=1)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def output_fn(predictions, content_type=\"text/csv\"):\n",
    "    \"\"\"\n",
    "    After invoking predict_fn, the model server invokes `output_fn`.\n",
    "    \"\"\"\n",
    "    print (f'Output, Content_type: {content_type}')\n",
    "    \n",
    "    if content_type == \"text/csv\":\n",
    "        outputs = CSV_SERIALIZER.serialize(predictions)\n",
    "        print (outputs)\n",
    "        return Response(outputs, mimetype=content_type)\n",
    "\n",
    "    elif content_type == \"application/json\":\n",
    "\n",
    "        outputs = json.dumps({\n",
    "            'pred': predictions[0][0],\n",
    "            'prob': predictions[0][1]\n",
    "        })                \n",
    "        #return outputs\n",
    "        return Response(outputs, mimetype=content_type)\n",
    "    else:\n",
    "        raise ValueError(\"Content type {} is not supported.\".format(content_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb17072a-a90f-4dce-b46c-b5f452a0abf9",
   "metadata": {},
   "source": [
    "* 3.1.2 param setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4332ba35-ee03-4e63-9c6b-8c3994cb6925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sagemaker\n",
    "from sagemaker.model_monitor import DataCaptureConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17600ba2-f4b2-4149-88db-0f42a7c86e8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_mode = False\n",
    "strEndpointName = \"endpoint-cloud-DJ-SM-IMD-1682062443\" # None\n",
    "\n",
    "if local_mode:\n",
    "    \n",
    "    from sagemaker.local import LocalSession\n",
    "    \n",
    "    strInstanceType = \"local\"\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "    strDeployType = \"local\"\n",
    "        \n",
    "else:\n",
    "    strInstanceType = \"ml.m5.xlarge\"\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    strDeployType = \"cloud\"\n",
    "    \n",
    "strS3ModelPath = pm.get_params(key=\"-\".join([strPrefix, \"MODEL-PATH\"]))\n",
    "if strEndpointName == None:\n",
    "    strEndpointName = f\"endpoint-{strDeployType}-{strPrefix}-{int(time.time())}\" \n",
    "\n",
    "strS3DataCapturePath = os.path.join(\n",
    "    \"s3://{}\".format(strBucketName),\n",
    "    strPrefix,\n",
    "    \"monitor\",\n",
    "    \"data-capture\"\n",
    ")\n",
    "\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    sampling_percentage=100,\n",
    "    destination_s3_uri=strS3DataCapturePath,\n",
    "    capture_options=[\"REQUEST\", \"RESPONSE\"],\n",
    "    csv_content_types=[\"text/csv\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee9d940e-56fb-45d7-bc4b-13d4a9a65e18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strInstanceType: ml.m5.xlarge\n",
      "sagemaker_session: <sagemaker.session.Session object at 0x7fd92d08f8e0>\n",
      "strS3ModelPath: s3://sagemaker-us-east-1-419974056037/DJ-SM-IMD/training/model-output/DJ-SM-IMD-experiments-0419-04191681877971/output/model.tar.gz\n",
      "strEndpointName: endpoint-cloud-DJ-SM-IMD-1682062443\n"
     ]
    }
   ],
   "source": [
    "print (f'strInstanceType: {strInstanceType}')\n",
    "print (f'sagemaker_session: {sagemaker_session}')\n",
    "print (f'strS3ModelPath: {strS3ModelPath}')\n",
    "print (f'strEndpointName: {strEndpointName}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c1c5cc-1b4e-4ee0-9a4b-60d66fdfcc48",
   "metadata": {},
   "source": [
    "* Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b83cbd8-652a-4f0c-a737-77f7bd23a2d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.xgboost.model import XGBoostModel\n",
    "from sagemaker.serializers import CSVSerializer, NumpySerializer, JSONSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer, JSONDeserializer, NumpyDeserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7312bd2c-ca02-40a8-97fa-60658d964fab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgb_model = XGBoostModel(\n",
    "    model_data=strS3ModelPath,\n",
    "    role=strExecutionRole,\n",
    "    source_dir=\"./source/deploy\",\n",
    "    entry_point=\"inference.py\",\n",
    "    framework_version=\"1.3-1\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761b0c74-527b-482f-a4a0-e9a40c27b767",
   "metadata": {},
   "source": [
    "* Create Endpoint with **data capture**\n",
    "    * SageMaker SDK는 `deploy(...)` 메소드를 호출 시, `create-endpoint-config`와 `create-endpoint`를 같이 수행합니다. 좀 더 세분화된 파라메터 조정을 원하면 AWS CLI나 boto3 SDK client 활용을 권장 드립니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "739afd11-29d5-46dd-8a6e-52ed883f4c5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\u001b[36mnj5in58f4q-algo-1-x7hac |\u001b[0m [2023-04-21 01:27:25 +0000] [73] [INFO] Handling signal: term\n",
      "\u001b[36mnj5in58f4q-algo-1-x7hac |\u001b[0m [2023-04-21 01:27:25 +0000] [79] [INFO] Worker exiting (pid: 79)\n",
      "\u001b[36mnj5in58f4q-algo-1-x7hac |\u001b[0m [2023-04-21 01:27:25 +0000] [104] [INFO] Worker exiting (pid: 104)\n",
      "\u001b[36mnj5in58f4q-algo-1-x7hac |\u001b[0m [2023-04-21 01:27:25 +0000] [87] [INFO] Worker exiting (pid: 87)\n",
      "\u001b[36mnj5in58f4q-algo-1-x7hac |\u001b[0m [2023-04-21 01:27:25 +0000] [88] [INFO] Worker exiting (pid: 88)\n",
      "\u001b[36mnj5in58f4q-algo-1-x7hac exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "-----!"
     ]
    }
   ],
   "source": [
    "xgb_predictor = xgb_model.deploy(\n",
    "    endpoint_name=strEndpointName,\n",
    "    instance_type=strInstanceType, \n",
    "    initial_instance_count=1,\n",
    "    data_capture_config=data_capture_config,\n",
    "    serializer=CSVSerializer(),\n",
    "    deserializer=CSVDeserializer(),\n",
    "    wait=True,\n",
    "    log=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2d466f-e6b2-4d3f-a177-03dfb84cca73",
   "metadata": {
    "tags": []
   },
   "source": [
    "* inference (based on **boto3**)\n",
    "    - **boto3 기반 invocation시 runtime_client가 필요**\n",
    "    - deploy 시 설정했던 \"serialization, deserialization\"이 적용되지 않음, 즉, **serialization, deserialization을 manually 해 줘야 함**\n",
    "        - 번거로울 수 있으나 de/serialization에 대한 자유도가 높음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c4bae9d-b760-451e-993d-3a4c21cafbf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c66dea0a-2ffb-498d-803e-c1f7c539a9c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime_client: <botocore.client.SageMakerRuntime object at 0x7fd92be4f010>\n"
     ]
    }
   ],
   "source": [
    "if \"local\" in strInstanceType: runtime_client = sagemaker.local.LocalSagemakerRuntimeClient()    \n",
    "else: runtime_client = boto3.Session().client('sagemaker-runtime')\n",
    "print (f'runtime_client: {runtime_client}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2db5e56-b80d-42b0-9d1b-830efe43c009",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdTest = pd.read_csv(f'{strLocalDataPath}/test.csv')\n",
    "pdTest = pdTest.drop('fraud', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dd4de0-dedd-451f-ba98-72a2815b77b0",
   "metadata": {},
   "source": [
    "* serialzaiton (csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4125ab68-b0a3-4637-844c-10cd8127d60e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csv_serializer = CSVSerializer()\n",
    "csv_deserializer = CSVDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4acfa015-ebd4-427c-ad4a-f606d924e6ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = csv_serializer.serialize(pdTest.values[122, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ed48dbd-12e8-4181-9dd1-54adc7c14209",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('7738.118217725296,11938.118217725296,39.0,61.0,0.0,1.0,750.0,3000.0,95815.0,2015.0,2.0,0.0,1.0,4200.0,12.0,6.0,4.0,22.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0',\n",
       " 'endpoint-cloud-DJ-SM-IMD-1682062443')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload,strEndpointName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da9d5bb8-f443-4333-9fe2-03988679c242",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20726663, 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=strEndpointName, \n",
    "    ContentType='text/csv',\n",
    "    Accept='text/csv',\n",
    "    Body=payload\n",
    ")\n",
    "pred = np.array(\n",
    "    csv_deserializer.deserialize(\n",
    "        stream=response['Body'],\n",
    "        content_type=\"text/csv\"\n",
    "    ),\n",
    "    dtype=np.float32\n",
    ")\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3a6387-a97d-4e2c-a99c-203dbd1ff57f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. View captured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f52f86dc-2fa9-4dc5-a7a4-79fec3939be3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fa3c7a9-f833-46ed-a94c-7146bcd6a307",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.Session().client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24aa9745-9d84-4b37-b633-94279e2db70d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_obj_body(obj_key, strBucketName):\n",
    "    return s3_client.get_object(Bucket=strBucketName, Key=obj_key).get(\"Body\").read().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22876e8e-8f02-43ef-89f8-782a81964c0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Capture Files:\n",
      "DJ-SM-IMD/monitor/data-capture/endpoint-cloud-DJ-SM-IMD-1682062443/AllTraffic/2023/04/21/07/37-45-420-13675920-9041-43dc-ba75-30351ca76e99.jsonl\n",
      " DJ-SM-IMD/monitor/data-capture/endpoint-cloud-DJ-SM-IMD-1682062443/AllTraffic/2023/04/21/07/40-14-247-5cb76178-c578-4215-8f08-33e9b164eb8f.jsonl\n",
      "DJ-SM-IMD/monitor/data-capture/endpoint-cloud-DJ-SM-IMD-1682062443/AllTraffic/2023/04/21/07\n"
     ]
    }
   ],
   "source": [
    "current_endpoint_capture_prefix = os.path.join(\n",
    "    strPrefix,\n",
    "    \"monitor\",\n",
    "    \"data-capture\",\n",
    "    strEndpointName\n",
    ")\n",
    "result = s3_client.list_objects(Bucket=strBucketName, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get(\"Contents\")]\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files))\n",
    "print (capture_files[len(capture_files) - 1][:capture_files[len(capture_files) - 1].rfind(\"/\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e769ff9d-d7f6-418d-b194-b96a9109615c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"captureData\": {\n",
      "    \"endpointInput\": {\n",
      "      \"observedContentType\": \"text/csv\",\n",
      "      \"mode\": \"INPUT\",\n",
      "      \"data\": \"17047.719421914477,28347.719421914484,52.0,51.0,0.0,1.0,750.0,2650.0,94601.0,2020.0,1.0,0.0,0.0,11300.0,1.0,21.0,0.0,11.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0\",\n",
      "      \"encoding\": \"CSV\"\n",
      "    },\n",
      "    \"endpointOutput\": {\n",
      "      \"observedContentType\": \"text/csv; charset=utf-8\",\n",
      "      \"mode\": \"OUTPUT\",\n",
      "      \"data\": \"0.15399029850959778,0.0\",\n",
      "      \"encoding\": \"CSV\"\n",
      "    }\n",
      "  },\n",
      "  \"eventMetadata\": {\n",
      "    \"eventId\": \"28ee34cc-db62-4222-ac13-a7993441f28c\",\n",
      "    \"inferenceTime\": \"2023-04-21T07:40:29Z\"\n",
      "  },\n",
      "  \"eventVersion\": \"0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "capture_file = get_obj_body(capture_files[-1], strBucketName)\n",
    "#print(capture_file.split(\"\\n\")[-2])\n",
    "print(json.dumps(json.loads(capture_file.split(\"\\n\")[-2]), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89efb2fe-b30a-461a-9b6e-a1848312002b",
   "metadata": {},
   "source": [
    "## 5. Model Monitor - Baselining and continuous monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff222fe-7c6b-4560-b485-acbaa3581d9a",
   "metadata": {},
   "source": [
    "### 5.1 Execute predictions using the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79047bbc-7dcd-4e35-937b-1978d0e9640e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7fa2e62c-7313-4776-82cd-0a3258b0f31b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdTest = pd.read_csv(f'{strLocalDataPath}/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9700ee11-b61d-43c4-abcb-bacbe23c16b9",
   "metadata": {},
   "source": [
    "* **[중요]** BinaryClassification에서 lable 및 prediction은 **integer** 형태여야 함\n",
    "\n",
    "* Notice the new attribute `inferenceId`, which we're setting when invoking the endpoint. This is used to join the prediction data with the ground truth data.\n",
    "    - You can check this in jsonl file (data captured)\n",
    "    - inferenceId attribute that is set as part of the invoke_endpoint call. If this is present, it will be used to join with ground truth data (otherwise `eventId` will be used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9456b99d-047a-4df0-b03a-b15b9ebe5425",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime_client: <botocore.client.SageMakerRuntime object at 0x7fd92b020850>\n",
      "0 0.13204898 0\n",
      "01 0.055153064 0\n",
      "2 0.5491993 1\n",
      "3 0.1765169 0\n",
      "4 0.5688996 1\n",
      "5 0.29007915 0\n",
      "6 0.5657198 1\n",
      "7 0.5151643 1\n",
      "8 0.7608245 1\n",
      "9 0.1539903 0\n",
      "10 0.56986153 1\n",
      "1011 0.6299357 1\n",
      "12 0.022774024 0\n",
      "13 0.1539903 0\n",
      "14 0.6334661 1\n",
      "15 0.12232828 0\n",
      "16 0.29007915 0\n",
      "17 0.20726663 0\n",
      "18 0.1539903 0\n",
      "19 0.08634089 0\n",
      "20 0.4788133 0\n",
      "2021 0.12121122 0\n",
      "22 0.13121638 0\n",
      "23 0.28858143 0\n",
      "24 0.29007915 0\n",
      "25 0.5542213 1\n",
      "26 0.054573808 0\n",
      "27 0.1539903 0\n",
      "28 0.12232828 0\n",
      "29 0.71389216 1\n",
      "30 0.1539903 0\n",
      "3031 0.15050104 0\n",
      "32 0.12232828 0\n",
      "33 0.22885583 0\n",
      "34 0.13088676 0\n",
      "35 0.5196692 1\n",
      "36 0.19419017 0\n",
      "37 0.5239408 1\n",
      "38 0.20070212 0\n",
      "39 0.5607354 1\n",
      "40 0.055153064 0\n",
      "4041 0.6337742 1\n",
      "42 0.31524706 0\n",
      "43 0.6320183 1\n",
      "44 0.4788133 0\n",
      "45 0.10985424 0\n",
      "46 0.4788133 0\n",
      "47 0.029054698 0\n",
      "48 0.033639323 0\n",
      "49 0.1539903 0\n",
      "50 0.05553387 0\n",
      "5051 0.32956877 0\n",
      "52 0.71389216 1\n",
      "53 0.20726663 0\n",
      "54 0.05553387 0\n",
      "55 0.1721504 0\n",
      "56 0.20726663 0\n",
      "57 0.71389216 1\n",
      "58 0.4788133 0\n",
      "59 0.11870792 0\n",
      "60 0.21822508 0\n",
      "6061 0.20726663 0\n",
      "62 0.19161433 0\n",
      "63 0.4788133 0\n",
      "64 0.029054698 0\n",
      "65 0.100602 0\n",
      "66 0.4788133 0\n",
      "67 0.11557074 0\n",
      "68 0.25186133 0\n",
      "69 0.5438895 1\n",
      "70 0.20795044 0\n",
      "7071 0.12232828 0\n",
      "72 0.4788133 0\n",
      "73 0.16335124 0\n",
      "74 0.1539903 0\n",
      "75 0.1765169 0\n",
      "76 0.08634089 0\n",
      "77 0.050928652 0\n",
      "78 0.48554805 0\n",
      "79 0.1765169 0\n",
      "80 0.5438895 1\n",
      "8081 0.07439031 0\n",
      "82 0.7477829 1\n",
      "83 0.13204898 0\n",
      "84 0.7966594 1\n",
      "85 0.1539903 0\n",
      "86 0.4788133 0\n",
      "87 0.71389216 1\n",
      "88 0.12689278 0\n",
      "89 0.15362112 0\n",
      "90 0.22885583 0\n",
      "9091 0.3748442 0\n",
      "92 0.09158806 0\n",
      "93 0.7581083 1\n",
      "94 0.022774024 0\n",
      "95 0.1539903 0\n",
      "96 0.0788935 0\n",
      "97 0.5196692 1\n",
      "98 0.055153064 0\n",
      "99 0.24179675 0\n",
      "100 0.3628392 0\n",
      "100101 0.043181483 0\n",
      "102 0.2944952 0\n",
      "103 0.4788133 0\n",
      "104 0.1765169 0\n",
      "105 0.503124 1\n",
      "106 0.13327144 0\n",
      "107 0.24569048 0\n",
      "108 0.29007915 0\n",
      "109 0.7436948 1\n",
      "110 0.4788133 0\n",
      "110111 0.22885583 0\n",
      "112 0.37260023 0\n",
      "113 0.28641385 0\n",
      "114 0.4788133 0\n",
      "115 0.5699187 1\n",
      "116 0.12232828 0\n",
      "117 0.1539903 0\n",
      "118 0.5196692 1\n",
      "119 0.4788133 0\n",
      "120 0.4788133 0\n",
      "120121 0.29007915 0\n",
      "122 0.20726663 0\n",
      "123 0.34202763 0\n",
      "124 0.20726663 0\n",
      "125 0.014433085 0\n",
      "126 0.1539903 0\n",
      "127 0.24434309 0\n",
      "128 0.25968933 0\n",
      "129 0.5196692 1\n",
      "130 0.100602 0\n",
      "130131 0.055153064 0\n",
      "132 0.30654588 0\n",
      "133 0.769991 1\n",
      "134 0.13204898 0\n",
      "135 0.1539903 0\n",
      "136 0.33202168 0\n",
      "137 0.59258366 1\n",
      "138 0.34605783 0\n",
      "139 0.18516982 0\n",
      "140 0.1539903 0\n",
      "140141 0.022774024 0\n",
      "142 0.12232828 0\n",
      "143 0.029260889 0\n",
      "144 0.1539903 0\n",
      "145 0.20726663 0\n",
      "146 0.32361645 0\n",
      "147 0.019171273 0\n",
      "148 0.15864685 0\n",
      "149 0.081974454 0\n",
      "150 0.02894524 0\n",
      "150151 0.1539903 0\n",
      "152 0.4788133 0\n",
      "153 0.8295074 1\n",
      "154 0.5196692 1\n",
      "155 0.6299357 1\n",
      "156 0.12689278 0\n",
      "157 0.1539903 0\n",
      "158 0.2004998 0\n",
      "159 0.5196692 1\n",
      "160 0.22885583 0\n",
      "160161 0.07166963 0\n",
      "162 0.1539903 0\n",
      "163 0.6337742 1\n",
      "164 0.12232828 0\n",
      "165 0.1539903 0\n",
      "166 0.12232828 0\n",
      "167 0.45798665 0\n",
      "168 0.22885583 0\n",
      "169 0.67166334 1\n",
      "170 0.029260889 0\n",
      "170171 0.1765169 0\n",
      "172 0.41296518 0\n",
      "173 0.7477829 1\n",
      "174 0.13204898 0\n",
      "175 0.5866165 1\n",
      "176 0.030800315 0\n",
      "177 0.5862465 1\n",
      "178 0.3438628 0\n",
      "179 0.1539903 0\n",
      "180 0.10263922 0\n",
      "180181 0.78238696 1\n",
      "182 0.1765169 0\n",
      "183 0.019171273 0\n",
      "184 0.09054634 0\n",
      "185 0.074893415 0\n",
      "186 0.19627438 0\n",
      "187 0.4788133 0\n",
      "188 0.1539903 0\n",
      "189 0.074893415 0\n",
      "190 0.6337742 1\n",
      "190191 0.1539903 0\n",
      "192 0.043084566 0\n",
      "193 0.15213254 0\n",
      "194 0.039570138 0\n",
      "195 0.21822508 0\n",
      "196 0.054950844 0\n",
      "197 0.22885583 0\n",
      "198 0.1765169 0\n",
      "199 0.4788133 0\n",
      "200 0.4788133 0\n",
      "200CPU times: user 806 ms, sys: 43.7 ms, total: 850 ms\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if \"local\" in strInstanceType: runtime_client = sagemaker.local.LocalSagemakerRuntimeClient()    \n",
    "else: runtime_client = boto3.Session().client('sagemaker-runtime')\n",
    "print (f'runtime_client: {runtime_client}')\n",
    "\n",
    "\n",
    "csv_serializer = CSVSerializer()\n",
    "csv_deserializer = CSVDeserializer()\n",
    "nLimit = 200\n",
    "\n",
    "with open(f'{strLocalDataPath}/model_baseline_data.csv', \"w\") as baseline_file:\n",
    "    baseline_file.write(\"probability,prediction,label\\n\")  # header\n",
    "    for idx, row in enumerate(pdTest.values):    \n",
    "        nLabel, fFeatures = int(row[0]), row[1:]\n",
    "        payload = csv_serializer.serialize(fFeatures)\n",
    "        response = runtime_client.invoke_endpoint(\n",
    "            EndpointName=strEndpointName, \n",
    "            ContentType='text/csv',\n",
    "            Accept='text/csv',\n",
    "            Body=payload,\n",
    "            InferenceId=str(idx)\n",
    "        )\n",
    "\n",
    "        predictions = np.array(\n",
    "            csv_deserializer.deserialize(\n",
    "                stream=response['Body'],\n",
    "                content_type=\"text/csv\"\n",
    "            ),\n",
    "            dtype=np.float32\n",
    "        ).flatten()\n",
    "        fProb, nPred = predictions[0], int(predictions[1])\n",
    "        #print (idx, fProb, nPred)\n",
    "        if idx % 10 == 0: print(idx, end=\"\", flush=True)\n",
    "        if idx >= nLimit: break\n",
    "        baseline_file.write(f\"{fProb},{nPred},{nLabel}\\n\")\n",
    "        #print(\".\", end=\"\", flush=True)\n",
    "        sleep(0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9e4657-cc3a-4f3b-9967-f8621c20de09",
   "metadata": {},
   "source": [
    "### 5.2 Examine the predictions from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "556d6d45-5c44-4055-96d2-a7ae8646a42e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability,prediction,label\n",
      "0.13204897940158844,0,0\n",
      "0.05515306442975998,0,0\n",
      "0.5491992831230164,1,0\n",
      "0.17651690542697906,0,0\n",
      "0.5688995718955994,1,0\n",
      "0.29007914662361145,0,0\n",
      "0.5657197833061218,1,0\n",
      "0.515164315700531,1,0\n",
      "0.7608245015144348,1,0\n"
     ]
    }
   ],
   "source": [
    "!head $f'{strLocalDataPath}/model_baseline_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02545fe0-c93c-4a62-b35b-0561770678b7",
   "metadata": {},
   "source": [
    "### 5.3 Upload the predictions as a baseline dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1bafec03-dd3e-406b-a77f-cc871bb1e0c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strS3ModelBaselinePrefix: DJ-SM-IMD/monitor/baselining/model\n",
      "strS3ModelBaselineDataUri: s3://sagemaker-us-east-1-419974056037/DJ-SM-IMD/monitor/baselining/model/data\n",
      "strS3ModelBaselineResultsUri: s3://sagemaker-us-east-1-419974056037/DJ-SM-IMD/monitor/baselining/model/results\n"
     ]
    }
   ],
   "source": [
    "strS3ModelBaselinePrefix = os.path.join(\n",
    "    strPrefix,\n",
    "    \"monitor\",\n",
    "    \"baselining\",\n",
    "    \"model\"\n",
    ")\n",
    "strS3ModelBaselineDataPrefix = os.path.join(\n",
    "    strS3ModelBaselinePrefix,\n",
    "    \"data\"\n",
    ")\n",
    "strS3ModelBaselineResultsPrefix = os.path.join(\n",
    "    strS3ModelBaselinePrefix,\n",
    "    \"results\"\n",
    ")\n",
    "strS3ModelBaselineDataUri = os.path.join(\n",
    "    \"s3://{}\".format(strBucketName),\n",
    "    strS3ModelBaselineDataPrefix\n",
    ")\n",
    "strS3ModelBaselineResultsUri = os.path.join(\n",
    "    \"s3://{}\".format(strBucketName),\n",
    "    strS3ModelBaselineResultsPrefix\n",
    ")\n",
    "\n",
    "print (f'strS3ModelBaselinePrefix: {strS3ModelBaselinePrefix}')\n",
    "print (f'strS3ModelBaselineDataUri: {strS3ModelBaselineDataUri}')\n",
    "print (f'strS3ModelBaselineResultsUri: {strS3ModelBaselineResultsUri}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7d84d768-9ff8-4b0f-b2f1-dbe45b2c7bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_baseline_data_file = open(f'{strLocalDataPath}/model_baseline_data.csv', \"rb\")\n",
    "s3_key = os.path.join(strS3ModelBaselinePrefix, \"data\", \"model_baseline_data.csv\")\n",
    "boto3.Session().resource(\"s3\").Bucket(strBucketName).Object(s3_key).upload_fileobj(model_baseline_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce9a529-10c1-4956-9eb1-7ac96568d392",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.4 Create a baselining job with test dataset predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3e784eb-fda1-4578-a31c-98e22224bf0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import ModelQualityMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad17dc19-e85e-4f8e-b883-706234503da5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the model quality monitoring object\n",
    "model_quality_monitor = ModelQualityMonitor(\n",
    "    role=strExecutionRole,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=1800\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cbd15ee5-4169-4651-8a10-6cf27ef6e8f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2023-04-21-07-49-52-209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............................................................................!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sagemaker.processing.ProcessingJob at 0x7fd969936590>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute the baseline suggestion job.\n",
    "# You will specify problem type, in this case Binary Classification, and provide other required attributes.\n",
    "model_quality_monitor.suggest_baseline(\n",
    "    #job_name=baseline_job_name,\n",
    "    baseline_dataset=os.path.join(\n",
    "        strS3ModelBaselineDataUri,\n",
    "        \"model_baseline_data.csv\"\n",
    "    ),\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=strS3ModelBaselineResultsUri,\n",
    "    problem_type=\"BinaryClassification\",\n",
    "    inference_attribute=\"prediction\",\n",
    "    probability_attribute=\"probability\",\n",
    "    ground_truth_attribute=\"label\",\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7eb7d6-e637-4903-9e5a-1f5dec947ba6",
   "metadata": {},
   "source": [
    "### 5.5 Explore the results of the baselining job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67db8b13-98b6-42b6-be59-105eb782be4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_baseline_job = model_quality_monitor.latest_baselining_job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68aa5dd-8032-4904-a902-f72ed04c3a34",
   "metadata": {},
   "source": [
    "* View the metrics generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c834009d-3188-4204-99b5-ade2628283dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>confusion_matrix.0.0</th>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion_matrix.0.1</th>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion_matrix.1.0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion_matrix.1.1</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall.value</th>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall.standard_deviation</th>\n",
       "      <td>0.149666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision.value</th>\n",
       "      <td>0.095238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision.standard_deviation</th>\n",
       "      <td>0.009601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy.value</th>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy.standard_deviation</th>\n",
       "      <td>0.015993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_best_constant_classifier.value</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_best_constant_classifier.standard_deviation</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_best_constant_classifier.value</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_best_constant_classifier.standard_deviation</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_best_constant_classifier.value</th>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_best_constant_classifier.standard_deviation</th>\n",
       "      <td>0.003746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_positive_rate.value</th>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_positive_rate.standard_deviation</th>\n",
       "      <td>0.149666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_negative_rate.value</th>\n",
       "      <td>0.804124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_negative_rate.standard_deviation</th>\n",
       "      <td>0.016714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>false_positive_rate.value</th>\n",
       "      <td>0.195876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>false_positive_rate.standard_deviation</th>\n",
       "      <td>0.016714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>false_negative_rate.value</th>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>false_negative_rate.standard_deviation</th>\n",
       "      <td>0.149666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>receiver_operating_characteristic_curve.false_positive_rates</th>\n",
       "      <td>[0.0, 0.005154639175257732, 0.0103092783505154...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>receiver_operating_characteristic_curve.true_positive_rates</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_recall_curve.precisions</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_recall_curve.recalls</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc.value</th>\n",
       "      <td>0.856529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc.standard_deviation</th>\n",
       "      <td>0.024192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>au_prc.value</th>\n",
       "      <td>0.091858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>au_prc.standard_deviation</th>\n",
       "      <td>0.009714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f0_5.value</th>\n",
       "      <td>0.114943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f0_5.standard_deviation</th>\n",
       "      <td>0.011791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1.value</th>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1.standard_deviation</th>\n",
       "      <td>0.018114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f2.value</th>\n",
       "      <td>0.30303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f2.standard_deviation</th>\n",
       "      <td>0.040134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f0_5_best_constant_classifier.value</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f0_5_best_constant_classifier.standard_deviation</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_best_constant_classifier.value</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_best_constant_classifier.standard_deviation</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f2_best_constant_classifier.value</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f2_best_constant_classifier.standard_deviation</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                    0\n",
       "confusion_matrix.0.0                                                                              156\n",
       "confusion_matrix.0.1                                                                               38\n",
       "confusion_matrix.1.0                                                                                2\n",
       "confusion_matrix.1.1                                                                                4\n",
       "recall.value                                                                                 0.666667\n",
       "recall.standard_deviation                                                                    0.149666\n",
       "precision.value                                                                              0.095238\n",
       "precision.standard_deviation                                                                 0.009601\n",
       "accuracy.value                                                                                    0.8\n",
       "accuracy.standard_deviation                                                                  0.015993\n",
       "recall_best_constant_classifier.value                                                             0.0\n",
       "recall_best_constant_classifier.standard_deviation                                                0.0\n",
       "precision_best_constant_classifier.value                                                          0.0\n",
       "precision_best_constant_classifier.standard_dev...                                                0.0\n",
       "accuracy_best_constant_classifier.value                                                          0.97\n",
       "accuracy_best_constant_classifier.standard_devi...                                           0.003746\n",
       "true_positive_rate.value                                                                     0.666667\n",
       "true_positive_rate.standard_deviation                                                        0.149666\n",
       "true_negative_rate.value                                                                     0.804124\n",
       "true_negative_rate.standard_deviation                                                        0.016714\n",
       "false_positive_rate.value                                                                    0.195876\n",
       "false_positive_rate.standard_deviation                                                       0.016714\n",
       "false_negative_rate.value                                                                    0.333333\n",
       "false_negative_rate.standard_deviation                                                       0.149666\n",
       "receiver_operating_characteristic_curve.false_p...  [0.0, 0.005154639175257732, 0.0103092783505154...\n",
       "receiver_operating_characteristic_curve.true_po...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "precision_recall_curve.precisions                   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "precision_recall_curve.recalls                      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "auc.value                                                                                    0.856529\n",
       "auc.standard_deviation                                                                       0.024192\n",
       "au_prc.value                                                                                 0.091858\n",
       "au_prc.standard_deviation                                                                    0.009714\n",
       "f0_5.value                                                                                   0.114943\n",
       "f0_5.standard_deviation                                                                      0.011791\n",
       "f1.value                                                                                     0.166667\n",
       "f1.standard_deviation                                                                        0.018114\n",
       "f2.value                                                                                      0.30303\n",
       "f2.standard_deviation                                                                        0.040134\n",
       "f0_5_best_constant_classifier.value                                                               0.0\n",
       "f0_5_best_constant_classifier.standard_deviation                                                  0.0\n",
       "f1_best_constant_classifier.value                                                                 0.0\n",
       "f1_best_constant_classifier.standard_deviation                                                    0.0\n",
       "f2_best_constant_classifier.value                                                                 0.0\n",
       "f2_best_constant_classifier.standard_deviation                                                    0.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_metrics = model_baseline_job.baseline_statistics().body_dict[\"binary_classification_metrics\"]\n",
    "pd.json_normalize(binary_metrics).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0b5e29-cdc5-48e4-9c81-394fcacd1e07",
   "metadata": {},
   "source": [
    "* View the constraints generated\n",
    "    - 베이스라인 만드는 데이테 셋에 대한 performance가 threshod가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b455bc2d-fe5e-4a29-b880-8f87824151f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>comparison_operator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>LessThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.095238</td>\n",
       "      <td>LessThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.8</td>\n",
       "      <td>LessThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_positive_rate</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>LessThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_negative_rate</th>\n",
       "      <td>0.804124</td>\n",
       "      <td>LessThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>false_positive_rate</th>\n",
       "      <td>0.195876</td>\n",
       "      <td>GreaterThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>false_negative_rate</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>GreaterThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc</th>\n",
       "      <td>0.856529</td>\n",
       "      <td>LessThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f0_5</th>\n",
       "      <td>0.114943</td>\n",
       "      <td>LessThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>LessThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f2</th>\n",
       "      <td>0.30303</td>\n",
       "      <td>LessThanThreshold</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    threshold   comparison_operator\n",
       "recall               0.666667     LessThanThreshold\n",
       "precision            0.095238     LessThanThreshold\n",
       "accuracy                  0.8     LessThanThreshold\n",
       "true_positive_rate   0.666667     LessThanThreshold\n",
       "true_negative_rate   0.804124     LessThanThreshold\n",
       "false_positive_rate  0.195876  GreaterThanThreshold\n",
       "false_negative_rate  0.333333  GreaterThanThreshold\n",
       "auc                  0.856529     LessThanThreshold\n",
       "f0_5                 0.114943     LessThanThreshold\n",
       "f1                   0.166667     LessThanThreshold\n",
       "f2                    0.30303     LessThanThreshold"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(model_baseline_job.suggested_constraints().body_dict[\"binary_classification_constraints\"]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cf694d-b23b-4447-bb82-c7816964ae40",
   "metadata": {},
   "source": [
    "## 6. Model Monitor - Setup continuous model monitoring to identify model quality drift\n",
    "\n",
    "In this section, you will setup a continuous model monitoring job that `monitors the quality of the deployed model` against the baseline generated in the previous section. This is to ensure that the quality does not degrade over time.\n",
    "\n",
    "In addition to the generated baseline, Amazon SageMaker Model Quality Monitoring `needs two additional inputs` - `predictions made by the deployed model endpoint` and the `ground truth data to be provided by the model` consuming application. Since you already enabled data capture on the endpoint, prediction data is captured in S3. The ground truth data depends on the what your model is predicting and what the business use case is. In this case, since the model is predicting customer churn, ground truth data may indicate if the customer actually left the company or not. For the purposes of this notebook, you will generate synthetic data as ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652940f6-9add-4dd8-94cc-4f4a02d8e862",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6.1 Generate prediction data for Model Quality Monitoring\n",
    "    - invocation 시 inferenceid를 전달함으로써 inference를 한 대상이 무엇인지 기록한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b105ef25-4459-48fe-8d57-d08a7a2389aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fa4afa0d-6cd8-42e2-9fbf-b539f9d80f37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdTest = pd.read_csv(f'{strLocalDataPath}/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b6450873-f3c6-46a0-9b4e-6b9ba8e7d86a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def invoke_endpoint(strEndpointName, pdDF):\n",
    "    for idx, row in enumerate(pdDF.values):    \n",
    "        nLabel, fFeatures = int(row[0]), row[1:]\n",
    "        payload = csv_serializer.serialize(fFeatures)\n",
    "\n",
    "        response = runtime_client.invoke_endpoint(\n",
    "            EndpointName=strEndpointName, \n",
    "            ContentType='text/csv',\n",
    "            Accept='text/csv',\n",
    "            Body=payload,\n",
    "            InferenceId=str(idx)\n",
    "        )\n",
    "        sleep(0.3)\n",
    "\n",
    "def invoke_endpoint_forever(strEndpointName, pdDF):\n",
    "    while True:\n",
    "        try:\n",
    "            invoke_endpoint(strEndpointName, pdDF)\n",
    "        except session.sagemaker_runtime_client.exceptions.ValidationError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b6f62da3-83d8-43db-9b00-987de8a0d6bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "thread = Thread(target=invoke_endpoint_forever, args=(strEndpointName, pdTest))\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ae957e-f142-4df2-bf81-118e03cbe3fe",
   "metadata": {},
   "source": [
    "### 6.2 View captured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6f70a7c1-1ba9-4695-9378-9d2491a3ff5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "62221461-c84d-491e-832c-919b7b312b6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.Session().client(\"s3\")\n",
    "def get_obj_body(obj_key, strBucketName):\n",
    "    return s3_client.get_object(Bucket=strBucketName, Key=obj_key).get(\"Body\").read().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "12a77667-1a6b-4a25-808d-e0e70fdba31d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Capture Files:\n",
      "DJ-SM-IMD/monitor/data-capture/endpoint-cloud-DJ-SM-IMD-1682062443/AllTraffic/2023/04/21/07/37-45-420-13675920-9041-43dc-ba75-30351ca76e99.jsonl\n",
      " DJ-SM-IMD/monitor/data-capture/endpoint-cloud-DJ-SM-IMD-1682062443/AllTraffic/2023/04/21/07/40-14-247-5cb76178-c578-4215-8f08-33e9b164eb8f.jsonl\n",
      " DJ-SM-IMD/monitor/data-capture/endpoint-cloud-DJ-SM-IMD-1682062443/AllTraffic/2023/04/21/07/46-27-464-796254fd-0ab0-4f76-85bd-68362138a32f.jsonl\n",
      " DJ-SM-IMD/monitor/data-capture/endpoint-cloud-DJ-SM-IMD-1682062443/AllTraffic/2023/04/21/07/47-27-652-9a5038f5-f8ff-403a-a1d6-d72bbfa9995f.jsonl\n",
      " DJ-SM-IMD/monitor/data-capture/endpoint-cloud-DJ-SM-IMD-1682062443/AllTraffic/2023/04/21/07/48-27-944-3348623f-06ca-4cdf-8e8e-15c54eab1f21.jsonl\n",
      " DJ-SM-IMD/monitor/data-capture/endpoint-cloud-DJ-SM-IMD-1682062443/AllTraffic/2023/04/21/07/57-13-311-53b1812c-3b87-4c39-8590-34d8feb2db78.jsonl\n",
      " DJ-SM-IMD/monitor/data-capture/endpoint-cloud-DJ-SM-IMD-1682062443/AllTraffic/2023/04/21/07/58-13-657-150e03f8-2e6d-4803-ad1a-98c85aeb66db.jsonl\n",
      " DJ-SM-IMD/monitor/data-capture/endpoint-cloud-DJ-SM-IMD-1682062443/AllTraffic/2023/04/21/07/59-13-748-484e8a29-eb3e-4bfc-bed4-be4f8fdb04bb.jsonl\n",
      " DJ-SM-IMD/monitor/data-capture/endpoint-cloud-DJ-SM-IMD-1682062443/AllTraffic/2023/04/21/08/00-13-835-6bb258d9-9dc8-40ad-af2c-cacef72d32e2.jsonl\n",
      " DJ-SM-IMD/monitor/data-capture/endpoint-cloud-DJ-SM-IMD-1682062443/AllTraffic/2023/04/21/08/01-13-873-46ce35c5-7326-43ce-ba35-4e6049c81834.jsonl\n",
      " DJ-SM-IMD/monitor/data-capture/endpoint-cloud-DJ-SM-IMD-1682062443/AllTraffic/2023/04/21/08/02-14-248-6be14bf5-2ffa-48f0-a385-3fce672625e2.jsonl\n",
      " DJ-SM-IMD/monitor/data-capture/endpoint-cloud-DJ-SM-IMD-1682062443/AllTraffic/2023/04/21/08/03-14-555-b38a8a07-2aba-4f81-84bf-6a095caceaad.jsonl\n",
      "DJ-SM-IMD/monitor/data-capture/endpoint-cloud-DJ-SM-IMD-1682062443/AllTraffic/2023/04/21/08\n"
     ]
    }
   ],
   "source": [
    "current_endpoint_capture_prefix = os.path.join(\n",
    "    strPrefix,\n",
    "    \"monitor\",\n",
    "    \"data-capture\",\n",
    "    strEndpointName\n",
    ")\n",
    "result = s3_client.list_objects(Bucket=strBucketName, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get(\"Contents\")]\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files))\n",
    "print (capture_files[len(capture_files) - 1][:capture_files[len(capture_files) - 1].rfind(\"/\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "42c0148e-891b-47a0-bf81-60ed84795d3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"captureData\": {\n",
      "    \"endpointInput\": {\n",
      "      \"observedContentType\": \"text/csv\",\n",
      "      \"mode\": \"INPUT\",\n",
      "      \"data\": \"34432.685006666194,47132.685006666194,32.0,55.0,0.0,1.0,750.0,3000.0,85705.0,2019.0,3.0,1.0,1.0,12700.0,10.0,9.0,2.0,10.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,1.0\",\n",
      "      \"encoding\": \"CSV\"\n",
      "    },\n",
      "    \"endpointOutput\": {\n",
      "      \"observedContentType\": \"text/csv; charset=utf-8\",\n",
      "      \"mode\": \"OUTPUT\",\n",
      "      \"data\": \"0.2885814309120178,0.0\",\n",
      "      \"encoding\": \"CSV\"\n",
      "    }\n",
      "  },\n",
      "  \"eventMetadata\": {\n",
      "    \"eventId\": \"13e6b883-d628-4cd5-92d8-696105e287d7\",\n",
      "    \"inferenceId\": \"968\",\n",
      "    \"inferenceTime\": \"2023-04-21T08:03:14Z\"\n",
      "  },\n",
      "  \"eventVersion\": \"0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "capture_file = get_obj_body(capture_files[-1], strBucketName)\n",
    "print(json.dumps(json.loads(capture_file.split(\"\\n\")[1]), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f567212-4c01-4e79-a69a-3b083056b4f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6.3 Generate synthetic ground truth\n",
    "\n",
    "    * Next, start generating ground truth data. The model quality job will fail if there's no ground truth data to merge.\n",
    "    * \"6-1\" 과정에서 inference 한 대상에 대한 ground truth (정답)을 `임의로` 생성해 준다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ded0962c-529d-4311-ac3a-13468563166c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from time import strftime, gmtime\n",
    "from datetime import datetime\n",
    "from sagemaker.s3 import S3Uploader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "656ef36a-7f85-4d0b-8705-d005fc7c38ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strS3GrondTruthPath: s3://sagemaker-us-east-1-419974056037/DJ-SM-IMD/monitor/ground-truth/model/2023-04-21-07-57-57\n"
     ]
    }
   ],
   "source": [
    "strS3GrondTruthPath=os.path.join(\n",
    "    \"s3://{}\".format(strBucketName),\n",
    "    strPrefix,\n",
    "    \"monitor\",\n",
    "    \"ground-truth\",\n",
    "    \"model\",\n",
    "    strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    ")\n",
    "print (f'strS3GrondTruthPath: {strS3GrondTruthPath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "43bf6bff-3e2e-4e74-a8cc-11b81f58fd05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ground_truth_with_id(inference_id):\n",
    "    random.seed(inference_id)  # to get consistent results\n",
    "    rand = random.random()\n",
    "    return {\n",
    "        \"groundTruthData\": {\n",
    "            \"data\": \"1\" if rand < 0.7 else \"0\",  # randomly generate positive labels 70% of the time\n",
    "            \"encoding\": \"CSV\",\n",
    "        },\n",
    "        \"eventMetadata\": {\n",
    "            \"eventId\": str(inference_id),\n",
    "        },\n",
    "        \"eventVersion\": \"0\",\n",
    "    }\n",
    "\n",
    "def upload_ground_truth(records, upload_time):\n",
    "    fake_records = [json.dumps(r) for r in records]\n",
    "    data_to_upload = \"\\n\".join(fake_records)\n",
    "    target_s3_uri = f\"{strS3GrondTruthPath}/{upload_time:%Y/%m/%d/%H/%M%S}.jsonl\"\n",
    "    print(f\"Uploading {len(fake_records)} records to\", target_s3_uri)\n",
    "    S3Uploader.upload_string_as_file_body(data_to_upload, target_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8fb12714-3d86-4daa-9e35-22ef7a4def33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading 1000 records to s3://sagemaker-us-east-1-419974056037/DJ-SM-IMD/monitor/ground-truth/model/2023-04-21-07-57-57/2023/04/21/07/5802.jsonl\n"
     ]
    }
   ],
   "source": [
    "NUM_GROUND_TRUTH_RECORDS = 1000  # 1000 are the number of rows in data we're sending for inference\n",
    "\n",
    "fake_records = [ground_truth_with_id(i) for i in range(NUM_GROUND_TRUTH_RECORDS)]\n",
    "upload_ground_truth(fake_records, datetime.utcnow())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ad572d-23f4-475c-a890-1774c235870c",
   "metadata": {},
   "source": [
    "### 6.4 Create a monitoring schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ed6798aa-120f-40b5-8753-1fa630be2d49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import EndpointInput\n",
    "from sagemaker.model_monitor import CronExpressionGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ab130f13-76e5-4465-b0aa-6c79339449a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mon_model_schedule_name = \"DEMO-model-drift-monitor-schedule-\" + strftime(\n",
    "    \"%Y-%m-%d-%H-%M-%S\", gmtime()\n",
    ")\n",
    "\n",
    "strS3ModelReportPath = os.path.join(\n",
    "    \"s3://{}\".format(strBucketName),\n",
    "    strPrefix,\n",
    "    \"monitor\",\n",
    "    \"report\",\n",
    "    \"model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00774a0f-d237-4bec-a68f-cef3cd39b087",
   "metadata": {
    "tags": []
   },
   "source": [
    "* Create an enpointInput\n",
    "    - For the monitoring schedule you need to specify how to interpret an endpoint's output. Given that the endpoint in this notebook outputs CSV data, the below code specifies that the `first column of the output, 0, contains a probability` (of churn in this example). You will further specify 0.5 as the cutoff used to determine a positive label (that is, predict that a customer will churn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7dacef33-e258-435b-9856-7e164852d703",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpointInput = EndpointInput(\n",
    "    endpoint_name=strEndpointName,\n",
    "    probability_attribute=\"0\",\n",
    "    probability_threshold_attribute=0.5,\n",
    "    destination=\"/opt/ml/processing/input_data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f5d172-3354-4b1f-a43b-91075fb72195",
   "metadata": {
    "tags": []
   },
   "source": [
    "* Create the monitoring schedule to execute every hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ae2a7b74-efc2-4bee-a383-7d7d7aa11f24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.model_monitor.model_monitoring:Creating Monitoring Schedule with name: DEMO-model-drift-monitor-schedule-2023-04-21-07-58-44\n"
     ]
    }
   ],
   "source": [
    "response = model_quality_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=mon_model_schedule_name,\n",
    "    endpoint_input=endpointInput,\n",
    "    output_s3_uri=strS3ModelReportPath,\n",
    "    problem_type=\"BinaryClassification\",\n",
    "    ground_truth_input=strS3GrondTruthPath,\n",
    "    constraints=model_baseline_job.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0d9cccc0-e59d-471d-84c6-ddc7e7d14ea9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MonitoringScheduleArn': 'arn:aws:sagemaker:us-east-1:419974056037:monitoring-schedule/demo-model-drift-monitor-schedule-2023-04-21-07-58-44',\n",
       " 'MonitoringScheduleName': 'DEMO-model-drift-monitor-schedule-2023-04-21-07-58-44',\n",
       " 'MonitoringScheduleStatus': 'Pending',\n",
       " 'MonitoringType': 'ModelQuality',\n",
       " 'CreationTime': datetime.datetime(2023, 4, 21, 7, 58, 52, 34000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2023, 4, 21, 7, 58, 52, 153000, tzinfo=tzlocal()),\n",
       " 'MonitoringScheduleConfig': {'ScheduleConfig': {'ScheduleExpression': 'cron(0 * ? * * *)'},\n",
       "  'MonitoringJobDefinitionName': 'model-quality-job-definition-2023-04-21-07-58-51-491',\n",
       "  'MonitoringType': 'ModelQuality'},\n",
       " 'EndpointName': 'endpoint-cloud-DJ-SM-IMD-1682062443',\n",
       " 'ResponseMetadata': {'RequestId': '30739e56-dc77-4540-9651-475e37f0b543',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '30739e56-dc77-4540-9651-475e37f0b543',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '619',\n",
       "   'date': 'Fri, 21 Apr 2023 07:58:55 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_quality_monitor.describe_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16fe6d9-3b1f-49cf-9cd3-cfe26649c727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13049f8-b6bc-4a05-a156-3415225bf2cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3abce59-1743-4b75-a607-b535a2fc1b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e5d6faf1-b989-48fe-a508-ef3a672dfec7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.monitoringjob_utils import run_model_monitor_job_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "92ce7036-4412-433e-8cd0-7ba0c19c2a3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'strS3BaselineResultsUri' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 16\u001b[0m\n\u001b[1;32m      9\u001b[0m data_capture_path \u001b[38;5;241m=\u001b[39m capture_files[\u001b[38;5;28mlen\u001b[39m(capture_files) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m][:capture_files[\u001b[38;5;28mlen\u001b[39m(capture_files) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mrfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m     10\u001b[0m strS3DataCapturePath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3://\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(strBucketName),\n\u001b[1;32m     12\u001b[0m     data_capture_path\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m strS3StatisticsPath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mstrS3BaselineResultsUri\u001b[49m,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatistics.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m strS3ConstraintsPath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     20\u001b[0m     strS3BaselineResultsUri,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstraints.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'strS3BaselineResultsUri' is not defined"
     ]
    }
   ],
   "source": [
    "current_endpoint_capture_prefix = os.path.join(\n",
    "    strPrefix,\n",
    "    \"monitor\",\n",
    "    \"data-capture\",\n",
    "    strEndpointName\n",
    ")\n",
    "result = s3_client.list_objects(Bucket=strBucketName, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get(\"Contents\")]\n",
    "data_capture_path = capture_files[len(capture_files) - 1][:capture_files[len(capture_files) - 1].rfind(\"/\")]\n",
    "strS3DataCapturePath = os.path.join(\n",
    "    \"s3://{}\".format(strBucketName),\n",
    "    data_capture_path\n",
    ")\n",
    "\n",
    "strS3StatisticsPath = os.path.join(\n",
    "    strS3BaselineResultsUri,\n",
    "    \"statistics.json\"\n",
    ")\n",
    "strS3ConstraintsPath = os.path.join(\n",
    "    strS3BaselineResultsUri,\n",
    "    \"constraints.json\"\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd8a7e2-fcef-45ad-871c-5a59e3dd252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'data_capture_path: {data_capture_path}')\n",
    "print (f'strS3BaselineResultsUri: {strS3BaselineResultsUri}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3a9112-2c65-458a-ade6-e4111962bebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = run_model_monitor_job_processor(\n",
    "    strRegionName,\n",
    "    \"ml.m5.xlarge\",\n",
    "    strExecutionRole,\n",
    "    strS3DataCapturePath,\n",
    "    strS3StatisticsPath,\n",
    "    strS3ConstraintsPath,\n",
    "    strS3ReportPath,\n",
    "    preprocessor_path=strS3CodePrepUri,\n",
    "    postprocessor_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c65b0f-0ebd-4f3e-aca1-b47b0154a6c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7f7bb8-1f66-4d2c-9e19-28e04947dc64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a380589-35a1-4ff0-8c01-61bbe6f445be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30bf065d-cf02-49f8-a93b-e21cd6eadd16",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.1 Constraint suggestion with baseline/training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86945d1b-3f51-4195-8011-7b97b5b4dc26",
   "metadata": {
    "tags": []
   },
   "source": [
    "* copy over the training dataset to Amazon S3 (if you already have it in Amazon S3, you could reuse it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6abd7f49-d22f-4632-b3a2-fb7ae7e228cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strS3BaselinePrefix: DJ-SM-IMD/monitor/baselining\n",
      "strS3BaselineDataUri: s3://sagemaker-us-east-1-419974056037/DJ-SM-IMD/monitor/baselining/data\n",
      "strS3BaselineResultsUri: s3://sagemaker-us-east-1-419974056037/DJ-SM-IMD/monitor/baselining/results\n"
     ]
    }
   ],
   "source": [
    "strS3BaselinePrefix = os.path.join(\n",
    "    strPrefix,\n",
    "    \"monitor\",\n",
    "    \"baselining\"\n",
    ")\n",
    "strS3BaselineDataPrefix = os.path.join(\n",
    "    strS3BaselinePrefix,\n",
    "    \"data\"\n",
    ")\n",
    "strS3BaselineResultsPrefix = os.path.join(\n",
    "    strS3BaselinePrefix,\n",
    "    \"results\"\n",
    ")\n",
    "strS3BaselineDataUri = os.path.join(\n",
    "    \"s3://{}\".format(strBucketName),\n",
    "    strS3BaselineDataPrefix\n",
    ")\n",
    "strS3BaselineResultsUri = os.path.join(\n",
    "    \"s3://{}\".format(strBucketName),\n",
    "    strS3BaselineResultsPrefix\n",
    ")\n",
    "\n",
    "print (f'strS3BaselinePrefix: {strS3BaselinePrefix}')\n",
    "print (f'strS3BaselineDataUri: {strS3BaselineDataUri}')\n",
    "print (f'strS3BaselineResultsUri: {strS3BaselineResultsUri}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9316d0aa-60dc-4086-8b01-bc54feae17f8",
   "metadata": {},
   "source": [
    "* change dtype\n",
    "    - 원하는 형태의 dtype으로 정의 할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1f7679fd-f1bc-4b2c-a243-a8c72418d5ce",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fraud                                   float64\n",
       "vehicle_claim                           float64\n",
       "total_claim_amount                      float64\n",
       "customer_age                            float64\n",
       "months_as_customer                      float64\n",
       "num_claims_past_year                    float64\n",
       "num_insurers_past_5_years               float64\n",
       "policy_deductable                       float64\n",
       "policy_annual_premium                   float64\n",
       "customer_zip                            float64\n",
       "auto_year                               float64\n",
       "num_vehicles_involved                   float64\n",
       "num_injuries                            float64\n",
       "num_witnesses                           float64\n",
       "injury_claim                            float64\n",
       "incident_month                          float64\n",
       "incident_day                            float64\n",
       "incident_dow                            float64\n",
       "incident_hour                           float64\n",
       "policy_state_AZ                         float64\n",
       "policy_state_CA                         float64\n",
       "policy_state_ID                         float64\n",
       "policy_state_NV                         float64\n",
       "policy_state_OR                         float64\n",
       "policy_state_WA                         float64\n",
       "policy_liability_100/200                float64\n",
       "policy_liability_15/30                  float64\n",
       "policy_liability_25/50                  float64\n",
       "policy_liability_30/60                  float64\n",
       "customer_gender_Female                  float64\n",
       "customer_gender_Male                    float64\n",
       "customer_gender_Other                   float64\n",
       "customer_gender_Unkown                  float64\n",
       "customer_education_Advanced Degree      float64\n",
       "customer_education_Associate            float64\n",
       "customer_education_Bachelor             float64\n",
       "customer_education_Below High School    float64\n",
       "customer_education_High School          float64\n",
       "driver_relationship_Child               float64\n",
       "driver_relationship_Other               float64\n",
       "driver_relationship_Self                float64\n",
       "driver_relationship_Spouse              float64\n",
       "driver_relationship_missing             float64\n",
       "incident_type_Break-in                  float64\n",
       "incident_type_Collision                 float64\n",
       "incident_type_Theft                     float64\n",
       "collision_type_Front                    float64\n",
       "collision_type_Rear                     float64\n",
       "collision_type_Side                     float64\n",
       "collision_type_missing                  float64\n",
       "incident_severity_Major                 float64\n",
       "incident_severity_Minor                 float64\n",
       "incident_severity_Totaled               float64\n",
       "authorities_contacted_Ambulance         float64\n",
       "authorities_contacted_Fire              float64\n",
       "authorities_contacted_None              float64\n",
       "authorities_contacted_Police            float64\n",
       "police_report_available_No              float64\n",
       "police_report_available_Yes             float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdTrain = pd.read_csv(f'{strLocalDataPath}/train.csv')\n",
    "dicDtypes = {}\n",
    "for strCol, dtype in zip(pdTrain.columns, pdTrain.dtypes):\n",
    "    strDtype = str(dtype)\n",
    "    if strDtype == \"int64\": dtype = np.float64 \n",
    "    dicDtypes[strCol] = dtype\n",
    "dicDtypes\n",
    "pdTrain = pd.read_csv(f'{strLocalDataPath}/train.csv', dtype=dicDtypes)\n",
    "pdTrain.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ac11ae-6768-473a-941b-43bc020c0a27",
   "metadata": {},
   "source": [
    "* upload train data to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7e246482-edda-478b-954e-7c4298879085",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_key = os.path.join(strS3BaselinePrefix, \"data\", \"train.csv\")\n",
    "\n",
    "with StringIO() as csv_buffer:\n",
    "    pdTrain.to_csv(csv_buffer, index=False, header=True)\n",
    "    response = s3_client.put_object(\n",
    "        Bucket=strBucketName, Key=s3_key, Body=csv_buffer.getvalue()\n",
    "    )\n",
    "# training_data_file = open(f'{strLocalDataPath}/train.csv', \"rb\")\n",
    "# s3_key = os.path.join(strS3BaselinePrefix, \"data\", \"train.csv\")\n",
    "# boto3.Session().resource(\"s3\").Bucket(strBucketName).Object(s3_key).upload_fileobj(training_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e6f255-866f-45f0-b0ea-401a2346bb4b",
   "metadata": {},
   "source": [
    "* Create a baselining job with training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6506fee9-5724-478c-88b8-d5c88ec74d00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ad4f98b0-e7c0-4f22-bb1d-da0779543ed2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: .\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2023-04-20-04-34-15-875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........................\u001b[34m2023-04-20 04:38:24,508 - matplotlib.font_manager - INFO - Generating new fontManager, this may take some time...\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:25.021952: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:25.021981: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:26.515282: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:26.515311: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:26.515335: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-0-143-102.ec2.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:26.515572: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:28,018 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:419974056037:processing-job/baseline-suggestion-job-2023-04-20-04-34-15-875', 'ProcessingJobName': 'baseline-suggestion-job-2023-04-20-04-34-15-875', 'Environment': {'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-us-east-1-419974056037/DJ-SM-IMD/monitor/baselining/data/train.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinition': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-us-east-1-419974056037/DJ-SM-IMD/monitor/baselining/results', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'RoleArn': 'arn:aws:iam::419974056037:role/service-role/AmazonSageMaker-ExecutionRole-20221206T163436', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:28,019 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:28,019 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:28,019 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:28,019 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:28,075 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:28,076 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:28,076 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'hosts': ['algo-1']}\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:28,082 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:28,083 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:28,083 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:28,520 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.143.102\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/sha\u001b[0m\n",
      "\u001b[34mre/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_362\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:28,530 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:28,533 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-bb2d0703-29af-4388-b3d7-6526a0201881\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,052 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,063 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,064 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,067 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,072 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,072 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,072 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,072 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,103 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,114 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,114 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,118 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,122 INFO blockmanagement.BlockManager: The block deletion will start around 2023 Apr 20 04:38:29\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,124 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,124 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,126 INFO util.GSet: 2.0% max memory 3.1 GB = 63.8 MB\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,126 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,168 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,171 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,171 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,171 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,171 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,172 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,172 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,172 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,172 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,172 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,172 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,172 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,197 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,197 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,197 INFO util.GSet: 1.0% max memory 3.1 GB = 31.9 MB\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,197 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,199 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,199 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,199 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,199 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,203 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,206 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,206 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,207 INFO util.GSet: 0.25% max memory 3.1 GB = 8.0 MB\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,207 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,245 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,245 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,245 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,248 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,248 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,249 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,249 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,250 INFO util.GSet: 0.029999999329447746% max memory 3.1 GB = 979.4 KB\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,250 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,271 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1082711963-10.0.143.102-1681965509265\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,281 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,288 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,366 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,378 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,381 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.143.102\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:29,392 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:31,448 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:31,448 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:33,525 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:33,525 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:35,616 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:35,617 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:37,716 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:37,717 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:39,878 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:39,879 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:49,887 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:51,440 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:51,797 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:51,834 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:51,843 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:52,271 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:52,295 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:52,295 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:52,295 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:52,296 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:52,317 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 3, script: , vendor: , memory -> name: memory, amount: 11536, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:52,330 INFO resource.ResourceProfile: Limiting resource is cpus at 3 tasks per executor\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:52,332 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:52,382 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:52,382 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:52,382 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:52,383 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:52,383 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:52,722 INFO util.Utils: Successfully started service 'sparkDriver' on port 40255.\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:52,751 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:52,786 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:52,805 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:52,805 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:52,837 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:52,858 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-1f6f285b-6405-4b74-8dc7-f1dad2138c31\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:52,874 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:52,910 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:52,941 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.0.143.102:40255/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1681965532266\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:53,461 INFO client.RMProxy: Connecting to ResourceManager at /10.0.143.102:8032\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:54,267 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:54,268 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:54,274 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15731 MB per container)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:54,274 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:54,274 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:54,275 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:54,280 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:54,354 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:56,142 INFO yarn.Client: Uploading resource file:/tmp/spark-a60f66bd-d0c9-46fa-ba00-201b3465a093/__spark_libs__6867419428104562823.zip -> hdfs://10.0.143.102/user/root/.sparkStaging/application_1681965515219_0001/__spark_libs__6867419428104562823.zip\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:57,726 INFO yarn.Client: Uploading resource file:/tmp/spark-a60f66bd-d0c9-46fa-ba00-201b3465a093/__spark_conf__8629531784073023163.zip -> hdfs://10.0.143.102/user/root/.sparkStaging/application_1681965515219_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:57,773 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:57,773 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:57,773 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:57,773 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:57,773 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:57,802 INFO yarn.Client: Submitting application application_1681965515219_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:58,007 INFO impl.YarnClientImpl: Submitted application application_1681965515219_0001\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:59,016 INFO yarn.Client: Application report for application_1681965515219_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:38:59,020 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: AM container is launched, waiting for AM container to Register with RM\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1681965537902\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1681965515219_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:00,022 INFO yarn.Client: Application report for application_1681965515219_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:01,030 INFO yarn.Client: Application report for application_1681965515219_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:02,033 INFO yarn.Client: Application report for application_1681965515219_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:02,895 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1681965515219_0001), /proxy/application_1681965515219_0001\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:03,037 INFO yarn.Client: Application report for application_1681965515219_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:03,038 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.143.102\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1681965537902\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1681965515219_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:03,040 INFO cluster.YarnClientSchedulerBackend: Application application_1681965515219_0001 has started running.\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:03,049 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45637.\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:03,049 INFO netty.NettyBlockTransferService: Server created on 10.0.143.102:45637\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:03,051 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:03,061 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.143.102, 45637, None)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:03,065 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.143.102:45637 with 1458.6 MiB RAM, BlockManagerId(driver, 10.0.143.102, 45637, None)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:03,068 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.143.102, 45637, None)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:03,069 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.143.102, 45637, None)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:03,228 INFO util.log: Logging initialized @13140ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:04,438 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:08,808 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.143.102:45226) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:08,987 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:40231 with 5.8 GiB RAM, BlockManagerId(1, algo-1, 40231, None)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:23,359 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:23,584 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:23,630 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:23,634 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:24,681 INFO datasources.InMemoryFileIndex: It took 38 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:24,843 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:25,127 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:25,130 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.143.102:45637 (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:25,134 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:25,471 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:25,474 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:25,477 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 1137905\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:25,527 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:25,542 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:25,543 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:25,543 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:25,545 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:25,550 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:25,587 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:25,593 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:25,594 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.143.102:45637 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:25,595 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:25,612 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:25,613 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:25,658 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4618 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:25,892 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:40231 (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:26,752 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:40231 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:27,131 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1488 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:27,133 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:27,141 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 1.560 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:27,145 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:27,146 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:27,148 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 1.620727 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:27,379 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.143.102:45637 in memory (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:27,384 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:40231 in memory (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:27,409 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.143.102:45637 in memory (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:27,414 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on algo-1:40231 in memory (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:29,444 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:29,445 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:29,449 INFO datasources.FileSourceStrategy: Output Data Schema: struct<fraud: string, vehicle_claim: string, total_claim_amount: string, customer_age: string, months_as_customer: string ... 57 more fields>\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:29,488 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:29,692 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:29,706 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:29,707 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.143.102:45637 (size: 39.1 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:29,708 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:100\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:29,720 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:29,757 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:100\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:29,759 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:100) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:29,759 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:100)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:29,759 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:29,761 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:29,762 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:100), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:29,810 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 28.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:29,812 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 11.1 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:29,813 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.143.102:45637 (size: 11.1 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:29,813 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:29,814 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:100) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:29,814 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:29,817 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:29,860 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:40231 (size: 11.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:30,923 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:40231 (size: 39.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:31,325 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:40231 (size: 588.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:31,467 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1652 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:31,467 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:31,469 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:100) finished in 1.704 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:31,469 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:31,470 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:31,470 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:100, took 1.712907 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:31,878 INFO codegen.CodeGenerator: Code generated in 306.693623 ms\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:32,523 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:32,526 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:32,527 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:32,527 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:32,529 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:32,532 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:32,550 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 124.5 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:32,552 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 37.5 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:32,553 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.143.102:45637 (size: 37.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:32,554 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:32,556 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:32,556 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:32,563 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:32,585 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:40231 (size: 37.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:33,622 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1061 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:33,622 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:33,625 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 1.090 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:33,627 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:33,628 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:33,628 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:33,629 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:33,708 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:33,710 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:33,711 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:33,711 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:33,711 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:33,712 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:33,725 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 177.4 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:33,728 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 49.2 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:33,729 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.143.102:45637 (size: 49.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:33,729 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:33,729 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:33,730 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:33,732 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:33,746 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:40231 (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:33,785 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:34,013 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 282 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:34,013 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:34,014 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.294 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:34,015 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:34,015 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:34,015 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.306493 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:34,059 INFO codegen.CodeGenerator: Code generated in 35.803427 ms\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:34,346 INFO codegen.CodeGenerator: Code generated in 31.393035 ms\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:34,412 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:34,413 INFO scheduler.DAGScheduler: Got job 4 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:34,414 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:34,415 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:34,416 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:34,417 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:34,445 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 48.2 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:34,448 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 19.3 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:34,448 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.143.102:45637 (size: 19.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:34,449 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:34,449 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:34,449 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:34,451 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:34,464 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:40231 (size: 19.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:34,915 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 464 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:34,916 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:34,916 INFO scheduler.DAGScheduler: ResultStage 5 (treeReduce at KLLRunner.scala:107) finished in 0.498 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:34,920 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:34,921 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:34,921 INFO scheduler.DAGScheduler: Job 4 finished: treeReduce at KLLRunner.scala:107, took 0.508765 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:34,937 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:40231 in memory (size: 11.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:34,973 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.143.102:45637 in memory (size: 11.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,018 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.143.102:45637 in memory (size: 37.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,019 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:40231 in memory (size: 37.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,041 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.143.102:45637 in memory (size: 49.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,041 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:40231 in memory (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,532 INFO codegen.CodeGenerator: Code generated in 108.492097 ms\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,540 INFO scheduler.DAGScheduler: Registering RDD 34 (collect at AnalysisRunner.scala:326) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,541 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,541 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,541 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,542 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,543 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,548 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 83.8 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,550 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 26.2 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,550 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.143.102:45637 (size: 26.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,551 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,552 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,552 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,553 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,569 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:40231 (size: 26.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,696 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 142 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,696 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,698 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326) finished in 0.153 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,698 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,700 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,701 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,701 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,857 INFO codegen.CodeGenerator: Code generated in 81.640288 ms\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,869 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,871 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,871 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,871 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,871 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,873 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,876 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 66.2 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,878 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,878 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.143.102:45637 (size: 19.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,879 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,879 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,880 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,881 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,918 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:40231 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:35,926 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:36,079 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 198 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:36,079 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:36,080 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:326) finished in 0.206 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:36,082 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:36,082 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:36,083 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.213218 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:36,148 INFO codegen.CodeGenerator: Code generated in 42.2674 ms\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:36,247 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:36,250 INFO scheduler.DAGScheduler: Registering RDD 45 (countByKey at ColumnProfiler.scala:592) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:36,251 INFO scheduler.DAGScheduler: Got job 7 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:36,251 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:36,251 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:36,251 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:36,254 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:36,263 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 40.5 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:36,266 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 17.2 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:36,266 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.143.102:45637 (size: 17.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:36,267 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:36,268 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:36,268 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:36,270 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:36,285 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:40231 (size: 17.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,423 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 1153 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,423 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,424 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (countByKey at ColumnProfiler.scala:592) finished in 1.169 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,424 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,424 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,424 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,424 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,425 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,427 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.1 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,430 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,431 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.143.102:45637 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,432 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,432 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,433 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,435 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,449 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:40231 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,454 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,482 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 48 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,482 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,482 INFO scheduler.DAGScheduler: ResultStage 10 (countByKey at ColumnProfiler.scala:592) finished in 0.057 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,483 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,483 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,483 INFO scheduler.DAGScheduler: Job 7 finished: countByKey at ColumnProfiler.scala:592, took 1.235778 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,666 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,666 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,666 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,666 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,667 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,668 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,672 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 93.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,674 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 30.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,674 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.143.102:45637 (size: 30.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,675 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,675 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,676 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,677 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,686 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:40231 (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,892 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 215 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,892 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,896 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.228 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,896 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,896 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,897 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,897 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,949 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,951 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,951 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,951 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,952 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,957 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,969 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 178.8 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,971 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 49.4 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,971 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.143.102:45637 (size: 49.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,972 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,972 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,972 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,974 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,984 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:40231 (size: 49.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:37,994 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,126 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 153 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,126 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,128 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.169 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,128 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,128 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,129 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.179516 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,278 INFO codegen.CodeGenerator: Code generated in 17.074903 ms\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,306 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,307 INFO scheduler.DAGScheduler: Got job 10 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,307 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,308 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,309 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,316 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,323 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 48.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,326 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 19.3 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,327 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.143.102:45637 (size: 19.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,328 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,328 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,328 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,329 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,352 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:40231 (size: 19.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,484 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 155 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,485 INFO scheduler.DAGScheduler: ResultStage 14 (treeReduce at KLLRunner.scala:107) finished in 0.168 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,494 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,495 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,495 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,495 INFO scheduler.DAGScheduler: Job 10 finished: treeReduce at KLLRunner.scala:107, took 0.188888 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,592 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.143.102:45637 in memory (size: 30.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,594 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:40231 in memory (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,651 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:40231 in memory (size: 17.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,679 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.143.102:45637 in memory (size: 17.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,749 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.143.102:45637 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,749 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:40231 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,832 INFO codegen.CodeGenerator: Code generated in 49.337312 ms\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,848 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.143.102:45637 in memory (size: 26.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,848 INFO scheduler.DAGScheduler: Registering RDD 69 (collect at AnalysisRunner.scala:326) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,849 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,849 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:40231 in memory (size: 26.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,850 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,850 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,852 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,852 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,861 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 83.8 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,863 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 26.2 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,863 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.143.102:45637 (size: 26.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,863 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,864 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,864 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,866 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,883 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:40231 (size: 26.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,900 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.143.102:45637 in memory (size: 19.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,906 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:40231 in memory (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,948 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.143.102:45637 in memory (size: 49.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:38,950 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:40231 in memory (size: 49.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,001 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.143.102:45637 in memory (size: 19.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,006 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:40231 in memory (size: 19.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,037 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 172 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,037 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,038 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326) finished in 0.185 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,038 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,039 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,039 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,039 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,077 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.143.102:45637 in memory (size: 19.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,078 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:40231 in memory (size: 19.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,154 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,156 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,157 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,157 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,157 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,159 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,161 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 66.2 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,171 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,177 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.143.102:45637 (size: 19.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,177 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,178 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,178 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,179 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,192 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:40231 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,197 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,212 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 32 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,212 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,213 INFO scheduler.DAGScheduler: ResultStage 17 (collect at AnalysisRunner.scala:326) finished in 0.053 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,214 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,214 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,215 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.060156 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,289 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,291 INFO scheduler.DAGScheduler: Registering RDD 80 (countByKey at ColumnProfiler.scala:592) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,292 INFO scheduler.DAGScheduler: Got job 13 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,293 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,293 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,293 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,294 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,305 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 40.5 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,308 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,309 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.143.102:45637 (size: 17.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,310 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,311 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,312 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,313 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,325 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:40231 (size: 17.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,396 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 83 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,396 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,397 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (countByKey at ColumnProfiler.scala:592) finished in 0.102 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,397 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,397 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,397 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 19)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,397 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,397 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,399 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.1 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,401 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,401 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.143.102:45637 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,403 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,404 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,404 INFO cluster.YarnScheduler: Adding task set 19.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,406 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 15) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,417 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:40231 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,421 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,433 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 15) in 28 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,434 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,434 INFO scheduler.DAGScheduler: ResultStage 19 (countByKey at ColumnProfiler.scala:592) finished in 0.036 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,435 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,435 INFO cluster.YarnScheduler: Killing all running tasks in stage 19: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,436 INFO scheduler.DAGScheduler: Job 13 finished: countByKey at ColumnProfiler.scala:592, took 0.146548 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,600 INFO scheduler.DAGScheduler: Registering RDD 86 (collect at AnalysisRunner.scala:326) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,601 INFO scheduler.DAGScheduler: Got map stage job 14 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,601 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,601 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,602 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,602 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,607 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 93.5 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,609 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 30.0 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,610 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.143.102:45637 (size: 30.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,610 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,611 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,611 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,612 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 16) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,623 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:40231 (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,877 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 16) in 265 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,878 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326) finished in 0.275 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,880 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,880 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,880 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,881 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,881 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,923 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,924 INFO scheduler.DAGScheduler: Got job 15 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,924 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,924 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,924 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,925 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,930 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 178.6 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,932 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 49.4 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,932 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.143.102:45637 (size: 49.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,933 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,933 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,933 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,934 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,942 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:40231 (size: 49.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:39,952 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,053 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 119 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,053 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,055 INFO scheduler.DAGScheduler: ResultStage 22 (collect at AnalysisRunner.scala:326) finished in 0.130 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,055 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,055 INFO cluster.YarnScheduler: Killing all running tasks in stage 22: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,056 INFO scheduler.DAGScheduler: Job 15 finished: collect at AnalysisRunner.scala:326, took 0.132394 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,196 INFO codegen.CodeGenerator: Code generated in 13.64208 ms\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,229 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,230 INFO scheduler.DAGScheduler: Got job 16 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,230 INFO scheduler.DAGScheduler: Final stage: ResultStage 23 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,230 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,231 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,231 INFO scheduler.DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,237 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 48.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,239 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 19.3 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,239 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.143.102:45637 (size: 19.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,240 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,240 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,241 INFO cluster.YarnScheduler: Adding task set 23.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,242 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 18) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,255 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:40231 (size: 19.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,433 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 18) in 191 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,433 INFO cluster.YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,435 INFO scheduler.DAGScheduler: ResultStage 23 (treeReduce at KLLRunner.scala:107) finished in 0.203 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,436 INFO scheduler.DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,437 INFO cluster.YarnScheduler: Killing all running tasks in stage 23: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,438 INFO scheduler.DAGScheduler: Job 16 finished: treeReduce at KLLRunner.scala:107, took 0.208715 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,764 INFO codegen.CodeGenerator: Code generated in 137.087171 ms\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,811 INFO scheduler.DAGScheduler: Registering RDD 104 (collect at AnalysisRunner.scala:326) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,811 INFO scheduler.DAGScheduler: Got map stage job 17 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,811 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,811 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,812 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,812 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,816 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 83.8 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,840 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 26.2 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,841 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.143.102:45637 (size: 26.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,842 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,854 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,854 INFO cluster.YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,855 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.0.143.102:45637 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,856 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-1:40231 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,856 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 19) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,874 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:40231 (size: 26.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,897 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.143.102:45637 in memory (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,899 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:40231 in memory (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,925 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.0.143.102:45637 in memory (size: 49.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,928 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-1:40231 in memory (size: 49.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,977 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.0.143.102:45637 in memory (size: 30.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,979 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-1:40231 in memory (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,987 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 19) in 131 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,987 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,988 INFO scheduler.DAGScheduler: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326) finished in 0.174 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,988 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,988 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,988 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:40,988 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,016 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.143.102:45637 in memory (size: 17.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,020 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:40231 in memory (size: 17.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,082 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,083 INFO scheduler.DAGScheduler: Got job 18 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,085 INFO scheduler.DAGScheduler: Final stage: ResultStage 26 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,085 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,086 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,087 INFO scheduler.DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,090 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 66.2 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,110 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-1:40231 in memory (size: 19.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,376 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,377 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.0.143.102:45637 in memory (size: 19.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,377 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.143.102:45637 (size: 19.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,377 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,378 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,378 INFO cluster.YarnScheduler: Adding task set 26.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,379 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 20) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,384 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:40231 in memory (size: 26.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,387 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.143.102:45637 in memory (size: 26.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,394 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:40231 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,398 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,432 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 20) in 53 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,432 INFO cluster.YarnScheduler: Removed TaskSet 26.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,433 INFO scheduler.DAGScheduler: ResultStage 26 (collect at AnalysisRunner.scala:326) finished in 0.344 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,433 INFO scheduler.DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,433 INFO cluster.YarnScheduler: Killing all running tasks in stage 26: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,433 INFO scheduler.DAGScheduler: Job 18 finished: collect at AnalysisRunner.scala:326, took 0.350864 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,493 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,494 INFO scheduler.DAGScheduler: Registering RDD 115 (countByKey at ColumnProfiler.scala:592) as input to shuffle 8\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,494 INFO scheduler.DAGScheduler: Got job 19 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,494 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,494 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,495 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,496 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,502 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 40.5 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,504 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,505 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.143.102:45637 (size: 17.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,507 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,508 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,508 INFO cluster.YarnScheduler: Adding task set 27.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,509 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 21) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,519 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:40231 (size: 17.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,573 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 21) in 64 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,573 INFO cluster.YarnScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,574 INFO scheduler.DAGScheduler: ShuffleMapStage 27 (countByKey at ColumnProfiler.scala:592) finished in 0.078 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,574 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,575 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,575 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 28)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,575 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,575 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,577 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 5.1 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,580 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,582 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.143.102:45637 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,582 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,582 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,582 INFO cluster.YarnScheduler: Adding task set 28.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,584 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 22) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,594 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-1:40231 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,596 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,613 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 22) in 30 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,613 INFO cluster.YarnScheduler: Removed TaskSet 28.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,614 INFO scheduler.DAGScheduler: ResultStage 28 (countByKey at ColumnProfiler.scala:592) finished in 0.038 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,614 INFO scheduler.DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,614 INFO cluster.YarnScheduler: Killing all running tasks in stage 28: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,615 INFO scheduler.DAGScheduler: Job 19 finished: countByKey at ColumnProfiler.scala:592, took 0.121769 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,760 INFO scheduler.DAGScheduler: Registering RDD 121 (collect at AnalysisRunner.scala:326) as input to shuffle 9\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,760 INFO scheduler.DAGScheduler: Got map stage job 20 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,760 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,760 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,761 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,761 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,764 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 93.5 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,766 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 30.0 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,766 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.143.102:45637 (size: 30.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,767 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,767 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,767 INFO cluster.YarnScheduler: Adding task set 29.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,769 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 23) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,777 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-1:40231 (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,869 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 23) in 101 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,869 INFO cluster.YarnScheduler: Removed TaskSet 29.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,870 INFO scheduler.DAGScheduler: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326) finished in 0.108 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,870 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,870 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,870 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,870 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,904 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,906 INFO scheduler.DAGScheduler: Got job 21 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,906 INFO scheduler.DAGScheduler: Final stage: ResultStage 31 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,906 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,906 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,906 INFO scheduler.DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,911 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 178.5 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,913 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 49.3 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,913 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.143.102:45637 (size: 49.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,914 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,914 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,914 INFO cluster.YarnScheduler: Adding task set 31.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,916 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 24) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,924 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-1:40231 (size: 49.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:41,933 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,027 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 24) in 110 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,027 INFO cluster.YarnScheduler: Removed TaskSet 31.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,027 INFO scheduler.DAGScheduler: ResultStage 31 (collect at AnalysisRunner.scala:326) finished in 0.120 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,028 INFO scheduler.DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,029 INFO cluster.YarnScheduler: Killing all running tasks in stage 31: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,029 INFO scheduler.DAGScheduler: Job 21 finished: collect at AnalysisRunner.scala:326, took 0.124454 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,128 INFO codegen.CodeGenerator: Code generated in 20.672542 ms\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,166 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,167 INFO scheduler.DAGScheduler: Got job 22 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,167 INFO scheduler.DAGScheduler: Final stage: ResultStage 32 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,167 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,168 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,168 INFO scheduler.DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[134] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,181 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 48.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,183 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 19.3 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,184 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.143.102:45637 (size: 19.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,184 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,185 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[134] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,185 INFO cluster.YarnScheduler: Adding task set 32.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,186 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 32.0 (TID 25) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,198 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on algo-1:40231 (size: 19.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,348 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 32.0 (TID 25) in 162 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,348 INFO cluster.YarnScheduler: Removed TaskSet 32.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,349 INFO scheduler.DAGScheduler: ResultStage 32 (treeReduce at KLLRunner.scala:107) finished in 0.178 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,349 INFO scheduler.DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,349 INFO cluster.YarnScheduler: Killing all running tasks in stage 32: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,350 INFO scheduler.DAGScheduler: Job 22 finished: treeReduce at KLLRunner.scala:107, took 0.183517 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,523 INFO codegen.CodeGenerator: Code generated in 44.013358 ms\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,530 INFO scheduler.DAGScheduler: Registering RDD 139 (collect at AnalysisRunner.scala:326) as input to shuffle 10\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,531 INFO scheduler.DAGScheduler: Got map stage job 23 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,531 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 33 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,531 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,532 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,532 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 33 (MapPartitionsRDD[139] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,537 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 83.8 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,539 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 26.2 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,540 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.0.143.102:45637 (size: 26.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,540 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,540 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[139] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,540 INFO cluster.YarnScheduler: Adding task set 33.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,541 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 26) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,558 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on algo-1:40231 (size: 26.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,638 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 26) in 97 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,638 INFO cluster.YarnScheduler: Removed TaskSet 33.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,639 INFO scheduler.DAGScheduler: ShuffleMapStage 33 (collect at AnalysisRunner.scala:326) finished in 0.106 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,639 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,639 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,639 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,639 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,687 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,688 INFO scheduler.DAGScheduler: Got job 24 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,688 INFO scheduler.DAGScheduler: Final stage: ResultStage 35 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,688 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,688 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,689 INFO scheduler.DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[142] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,690 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 66.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,692 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,692 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.0.143.102:45637 (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,693 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,693 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[142] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,694 INFO cluster.YarnScheduler: Adding task set 35.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,695 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 35.0 (TID 27) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,708 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on algo-1:40231 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,712 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,717 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 35.0 (TID 27) in 22 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,717 INFO cluster.YarnScheduler: Removed TaskSet 35.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,718 INFO scheduler.DAGScheduler: ResultStage 35 (collect at AnalysisRunner.scala:326) finished in 0.029 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,718 INFO scheduler.DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,719 INFO cluster.YarnScheduler: Killing all running tasks in stage 35: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,719 INFO scheduler.DAGScheduler: Job 24 finished: collect at AnalysisRunner.scala:326, took 0.031701 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,800 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,801 INFO scheduler.DAGScheduler: Registering RDD 150 (countByKey at ColumnProfiler.scala:592) as input to shuffle 11\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,802 INFO scheduler.DAGScheduler: Got job 25 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,802 INFO scheduler.DAGScheduler: Final stage: ResultStage 37 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,803 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 36)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,803 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 36)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,804 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[150] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,812 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 40.5 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,814 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 17.2 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,815 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.0.143.102:45637 (size: 17.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,815 INFO spark.SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,816 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[150] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,816 INFO cluster.YarnScheduler: Adding task set 36.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,818 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 36.0 (TID 28) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,833 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on algo-1:40231 (size: 17.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,915 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 36.0 (TID 28) in 97 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,916 INFO cluster.YarnScheduler: Removed TaskSet 36.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,917 INFO scheduler.DAGScheduler: ShuffleMapStage 36 (countByKey at ColumnProfiler.scala:592) finished in 0.112 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,921 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,921 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,921 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 37)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,921 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,922 INFO scheduler.DAGScheduler: Submitting ResultStage 37 (ShuffledRDD[151] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,923 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 5.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,925 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,926 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.0.143.102:45637 (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,926 INFO spark.SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,927 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (ShuffledRDD[151] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,927 INFO cluster.YarnScheduler: Adding task set 37.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,928 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 37.0 (TID 29) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,943 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on algo-1:40231 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,949 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,968 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 37.0 (TID 29) in 40 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,968 INFO cluster.YarnScheduler: Removed TaskSet 37.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,969 INFO scheduler.DAGScheduler: ResultStage 37 (countByKey at ColumnProfiler.scala:592) finished in 0.047 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,969 INFO scheduler.DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,970 INFO cluster.YarnScheduler: Killing all running tasks in stage 37: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:42,970 INFO scheduler.DAGScheduler: Job 25 finished: countByKey at ColumnProfiler.scala:592, took 0.169840 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,132 INFO scheduler.DAGScheduler: Registering RDD 156 (collect at AnalysisRunner.scala:326) as input to shuffle 12\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,133 INFO scheduler.DAGScheduler: Got map stage job 26 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,133 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 38 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,133 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,133 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,134 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 38 (MapPartitionsRDD[156] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,137 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 93.5 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,139 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 30.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,140 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.0.143.102:45637 (size: 30.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,140 INFO spark.SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,141 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 38 (MapPartitionsRDD[156] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,141 INFO cluster.YarnScheduler: Adding task set 38.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,142 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 38.0 (TID 30) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,155 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on algo-1:40231 (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,286 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 38.0 (TID 30) in 143 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,286 INFO cluster.YarnScheduler: Removed TaskSet 38.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,289 INFO scheduler.DAGScheduler: ShuffleMapStage 38 (collect at AnalysisRunner.scala:326) finished in 0.154 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,289 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,290 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,290 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,290 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,324 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,325 INFO scheduler.DAGScheduler: Got job 27 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,325 INFO scheduler.DAGScheduler: Final stage: ResultStage 40 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,326 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 39)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,326 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,326 INFO scheduler.DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[159] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,335 INFO memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 178.6 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,340 INFO memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 49.2 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,344 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.0.143.102:45637 (size: 49.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,344 INFO spark.SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,345 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[159] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,345 INFO cluster.YarnScheduler: Adding task set 40.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,346 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 40.0 (TID 31) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,356 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on algo-1:40231 (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,363 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,420 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 40.0 (TID 31) in 74 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,421 INFO scheduler.DAGScheduler: ResultStage 40 (collect at AnalysisRunner.scala:326) finished in 0.094 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,421 INFO scheduler.DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,422 INFO cluster.YarnScheduler: Removed TaskSet 40.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,423 INFO cluster.YarnScheduler: Killing all running tasks in stage 40: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,423 INFO scheduler.DAGScheduler: Job 27 finished: collect at AnalysisRunner.scala:326, took 0.098516 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,556 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on algo-1:40231 in memory (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,559 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on 10.0.143.102:45637 in memory (size: 19.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,590 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on algo-1:40231 in memory (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,600 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on 10.0.143.102:45637 in memory (size: 49.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,616 INFO codegen.CodeGenerator: Code generated in 12.676901 ms\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,625 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on algo-1:40231 in memory (size: 17.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,631 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on 10.0.143.102:45637 in memory (size: 17.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,649 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,649 INFO scheduler.DAGScheduler: Got job 28 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,650 INFO scheduler.DAGScheduler: Final stage: ResultStage 41 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,650 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,650 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,651 INFO scheduler.DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[169] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,658 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on 10.0.143.102:45637 in memory (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,660 INFO memory.MemoryStore: Block broadcast_34 stored as values in memory (estimated size 48.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,662 INFO memory.MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 19.3 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,662 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.0.143.102:45637 (size: 19.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,663 INFO spark.SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,663 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 41 (MapPartitionsRDD[169] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,663 INFO cluster.YarnScheduler: Adding task set 41.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,665 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 41.0 (TID 32) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,674 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on algo-1:40231 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,685 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on algo-1:40231 (size: 19.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,743 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on 10.0.143.102:45637 in memory (size: 30.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,747 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on algo-1:40231 in memory (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,753 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on 10.0.143.102:45637 in memory (size: 17.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,769 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on algo-1:40231 in memory (size: 17.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,773 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.0.143.102:45637 in memory (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,776 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-1:40231 in memory (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,782 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 10.0.143.102:45637 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,787 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on algo-1:40231 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,798 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 10.0.143.102:45637 in memory (size: 49.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,804 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 41.0 (TID 32) in 140 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,807 INFO cluster.YarnScheduler: Removed TaskSet 41.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,808 INFO scheduler.DAGScheduler: ResultStage 41 (treeReduce at KLLRunner.scala:107) finished in 0.155 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,808 INFO scheduler.DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,808 INFO cluster.YarnScheduler: Killing all running tasks in stage 41: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,808 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on algo-1:40231 in memory (size: 49.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,808 INFO scheduler.DAGScheduler: Job 28 finished: treeReduce at KLLRunner.scala:107, took 0.159416 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,814 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on algo-1:40231 in memory (size: 19.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,820 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 10.0.143.102:45637 in memory (size: 19.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,825 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 10.0.143.102:45637 in memory (size: 30.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,834 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on algo-1:40231 in memory (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,836 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.0.143.102:45637 in memory (size: 26.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,837 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-1:40231 in memory (size: 26.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,849 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 10.0.143.102:45637 in memory (size: 26.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:43,850 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on algo-1:40231 in memory (size: 26.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,027 INFO codegen.CodeGenerator: Code generated in 79.193822 ms\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,040 INFO scheduler.DAGScheduler: Registering RDD 174 (collect at AnalysisRunner.scala:326) as input to shuffle 13\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,040 INFO scheduler.DAGScheduler: Got map stage job 29 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,040 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 42 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,040 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,040 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,041 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 42 (MapPartitionsRDD[174] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,045 INFO memory.MemoryStore: Block broadcast_35 stored as values in memory (estimated size 83.8 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,047 INFO memory.MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 26.2 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,047 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.0.143.102:45637 (size: 26.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,048 INFO spark.SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,048 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 42 (MapPartitionsRDD[174] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,048 INFO cluster.YarnScheduler: Adding task set 42.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,049 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 42.0 (TID 33) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,062 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on algo-1:40231 (size: 26.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,190 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 42.0 (TID 33) in 141 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,190 INFO cluster.YarnScheduler: Removed TaskSet 42.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,192 INFO scheduler.DAGScheduler: ShuffleMapStage 42 (collect at AnalysisRunner.scala:326) finished in 0.149 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,192 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,193 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,193 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,193 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,312 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,312 INFO scheduler.DAGScheduler: Got job 30 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,314 INFO scheduler.DAGScheduler: Final stage: ResultStage 44 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,314 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 43)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,314 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,314 INFO scheduler.DAGScheduler: Submitting ResultStage 44 (MapPartitionsRDD[177] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,316 INFO memory.MemoryStore: Block broadcast_36 stored as values in memory (estimated size 66.2 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,318 INFO memory.MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,321 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.0.143.102:45637 (size: 19.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,323 INFO spark.SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,324 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 44 (MapPartitionsRDD[177] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,324 INFO cluster.YarnScheduler: Adding task set 44.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,326 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 44.0 (TID 34) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,337 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on algo-1:40231 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,342 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 13 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,349 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 44.0 (TID 34) in 23 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,350 INFO cluster.YarnScheduler: Removed TaskSet 44.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,350 INFO scheduler.DAGScheduler: ResultStage 44 (collect at AnalysisRunner.scala:326) finished in 0.035 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,351 INFO scheduler.DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,351 INFO cluster.YarnScheduler: Killing all running tasks in stage 44: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,351 INFO scheduler.DAGScheduler: Job 30 finished: collect at AnalysisRunner.scala:326, took 0.039556 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,444 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,445 INFO scheduler.DAGScheduler: Registering RDD 185 (countByKey at ColumnProfiler.scala:592) as input to shuffle 14\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,445 INFO scheduler.DAGScheduler: Got job 31 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,445 INFO scheduler.DAGScheduler: Final stage: ResultStage 46 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,446 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 45)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,446 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 45)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,447 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 45 (MapPartitionsRDD[185] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,455 INFO memory.MemoryStore: Block broadcast_37 stored as values in memory (estimated size 40.5 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,457 INFO memory.MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,464 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.0.143.102:45637 (size: 17.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,464 INFO spark.SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,465 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 45 (MapPartitionsRDD[185] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,465 INFO cluster.YarnScheduler: Adding task set 45.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,466 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 45.0 (TID 35) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,478 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on algo-1:40231 (size: 17.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,536 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 45.0 (TID 35) in 71 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,536 INFO cluster.YarnScheduler: Removed TaskSet 45.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,536 INFO scheduler.DAGScheduler: ShuffleMapStage 45 (countByKey at ColumnProfiler.scala:592) finished in 0.088 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,537 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,537 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,537 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 46)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,537 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,537 INFO scheduler.DAGScheduler: Submitting ResultStage 46 (ShuffledRDD[186] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,539 INFO memory.MemoryStore: Block broadcast_38 stored as values in memory (estimated size 5.1 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,541 INFO memory.MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,541 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on 10.0.143.102:45637 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,542 INFO spark.SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,542 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (ShuffledRDD[186] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,543 INFO cluster.YarnScheduler: Adding task set 46.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,544 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 46.0 (TID 36) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,554 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on algo-1:40231 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,559 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 14 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,568 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 46.0 (TID 36) in 24 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,568 INFO cluster.YarnScheduler: Removed TaskSet 46.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,568 INFO scheduler.DAGScheduler: ResultStage 46 (countByKey at ColumnProfiler.scala:592) finished in 0.030 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,569 INFO scheduler.DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,569 INFO cluster.YarnScheduler: Killing all running tasks in stage 46: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,569 INFO scheduler.DAGScheduler: Job 31 finished: countByKey at ColumnProfiler.scala:592, took 0.125408 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,722 INFO scheduler.DAGScheduler: Registering RDD 191 (collect at AnalysisRunner.scala:326) as input to shuffle 15\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,722 INFO scheduler.DAGScheduler: Got map stage job 32 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,722 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 47 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,723 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,724 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,725 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 47 (MapPartitionsRDD[191] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,729 INFO memory.MemoryStore: Block broadcast_39 stored as values in memory (estimated size 93.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,731 INFO memory.MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 30.0 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,732 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on 10.0.143.102:45637 (size: 30.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,732 INFO spark.SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,733 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 47 (MapPartitionsRDD[191] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,733 INFO cluster.YarnScheduler: Adding task set 47.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,734 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 47.0 (TID 37) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,745 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on algo-1:40231 (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,881 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 47.0 (TID 37) in 147 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,881 INFO cluster.YarnScheduler: Removed TaskSet 47.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,882 INFO scheduler.DAGScheduler: ShuffleMapStage 47 (collect at AnalysisRunner.scala:326) finished in 0.156 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,882 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,882 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,882 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,882 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,917 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,918 INFO scheduler.DAGScheduler: Got job 33 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,918 INFO scheduler.DAGScheduler: Final stage: ResultStage 49 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,918 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 48)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,918 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,919 INFO scheduler.DAGScheduler: Submitting ResultStage 49 (MapPartitionsRDD[194] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,925 INFO memory.MemoryStore: Block broadcast_40 stored as values in memory (estimated size 179.0 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,926 INFO memory.MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 49.4 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,927 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on 10.0.143.102:45637 (size: 49.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,927 INFO spark.SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,928 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 49 (MapPartitionsRDD[194] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,928 INFO cluster.YarnScheduler: Adding task set 49.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,929 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 49.0 (TID 38) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,938 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on algo-1:40231 (size: 49.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:44,947 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 15 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,010 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 49.0 (TID 38) in 81 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,010 INFO cluster.YarnScheduler: Removed TaskSet 49.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,011 INFO scheduler.DAGScheduler: ResultStage 49 (collect at AnalysisRunner.scala:326) finished in 0.092 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,011 INFO scheduler.DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,012 INFO cluster.YarnScheduler: Killing all running tasks in stage 49: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,012 INFO scheduler.DAGScheduler: Job 33 finished: collect at AnalysisRunner.scala:326, took 0.095089 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,147 INFO codegen.CodeGenerator: Code generated in 17.197321 ms\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,180 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,181 INFO scheduler.DAGScheduler: Got job 34 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,181 INFO scheduler.DAGScheduler: Final stage: ResultStage 50 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,182 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,183 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,183 INFO scheduler.DAGScheduler: Submitting ResultStage 50 (MapPartitionsRDD[204] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,204 INFO memory.MemoryStore: Block broadcast_41 stored as values in memory (estimated size 48.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,205 INFO memory.MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 19.3 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,206 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on 10.0.143.102:45637 (size: 19.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,206 INFO spark.SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,210 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 50 (MapPartitionsRDD[204] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,210 INFO cluster.YarnScheduler: Adding task set 50.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,213 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 50.0 (TID 39) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,226 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on algo-1:40231 (size: 19.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,361 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 50.0 (TID 39) in 148 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,361 INFO cluster.YarnScheduler: Removed TaskSet 50.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,362 INFO scheduler.DAGScheduler: ResultStage 50 (treeReduce at KLLRunner.scala:107) finished in 0.173 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,362 INFO scheduler.DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,362 INFO cluster.YarnScheduler: Killing all running tasks in stage 50: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,362 INFO scheduler.DAGScheduler: Job 34 finished: treeReduce at KLLRunner.scala:107, took 0.182252 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,645 INFO codegen.CodeGenerator: Code generated in 55.815288 ms\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,653 INFO scheduler.DAGScheduler: Registering RDD 209 (collect at AnalysisRunner.scala:326) as input to shuffle 16\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,653 INFO scheduler.DAGScheduler: Got map stage job 35 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,653 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 51 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,653 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,653 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,654 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 51 (MapPartitionsRDD[209] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,657 INFO memory.MemoryStore: Block broadcast_42 stored as values in memory (estimated size 83.8 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,659 INFO memory.MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 26.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,659 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on 10.0.143.102:45637 (size: 26.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,660 INFO spark.SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,660 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 51 (MapPartitionsRDD[209] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,660 INFO cluster.YarnScheduler: Adding task set 51.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,663 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 51.0 (TID 40) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,677 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on algo-1:40231 (size: 26.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,813 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 51.0 (TID 40) in 150 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,813 INFO cluster.YarnScheduler: Removed TaskSet 51.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,814 INFO scheduler.DAGScheduler: ShuffleMapStage 51 (collect at AnalysisRunner.scala:326) finished in 0.160 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,814 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,814 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,814 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,814 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,924 INFO storage.BlockManagerInfo: Removed broadcast_41_piece0 on 10.0.143.102:45637 in memory (size: 19.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,924 INFO storage.BlockManagerInfo: Removed broadcast_41_piece0 on algo-1:40231 in memory (size: 19.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,930 INFO storage.BlockManagerInfo: Removed broadcast_35_piece0 on 10.0.143.102:45637 in memory (size: 26.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,936 INFO storage.BlockManagerInfo: Removed broadcast_35_piece0 on algo-1:40231 in memory (size: 26.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,940 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on algo-1:40231 in memory (size: 19.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,945 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on 10.0.143.102:45637 in memory (size: 19.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,948 INFO storage.BlockManagerInfo: Removed broadcast_40_piece0 on 10.0.143.102:45637 in memory (size: 49.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,949 INFO storage.BlockManagerInfo: Removed broadcast_40_piece0 on algo-1:40231 in memory (size: 49.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,951 INFO storage.BlockManagerInfo: Removed broadcast_38_piece0 on 10.0.143.102:45637 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,952 INFO storage.BlockManagerInfo: Removed broadcast_38_piece0 on algo-1:40231 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,956 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on 10.0.143.102:45637 in memory (size: 19.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,958 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on algo-1:40231 in memory (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,962 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,963 INFO scheduler.DAGScheduler: Got job 36 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,963 INFO scheduler.DAGScheduler: Final stage: ResultStage 53 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,963 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 52)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,964 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,968 INFO scheduler.DAGScheduler: Submitting ResultStage 53 (MapPartitionsRDD[212] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,968 INFO storage.BlockManagerInfo: Removed broadcast_37_piece0 on 10.0.143.102:45637 in memory (size: 17.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,969 INFO storage.BlockManagerInfo: Removed broadcast_37_piece0 on algo-1:40231 in memory (size: 17.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,970 INFO memory.MemoryStore: Block broadcast_43 stored as values in memory (estimated size 66.2 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,972 INFO memory.MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,973 INFO storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on 10.0.143.102:45637 (size: 19.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,973 INFO spark.SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,973 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 53 (MapPartitionsRDD[212] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,973 INFO cluster.YarnScheduler: Adding task set 53.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,975 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 53.0 (TID 41) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,978 INFO storage.BlockManagerInfo: Removed broadcast_42_piece0 on 10.0.143.102:45637 in memory (size: 26.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:45,988 INFO storage.BlockManagerInfo: Removed broadcast_42_piece0 on algo-1:40231 in memory (size: 26.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,003 INFO storage.BlockManagerInfo: Removed broadcast_39_piece0 on 10.0.143.102:45637 in memory (size: 30.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,004 INFO storage.BlockManagerInfo: Removed broadcast_39_piece0 on algo-1:40231 in memory (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,005 INFO storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on algo-1:40231 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,011 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 16 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,016 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 53.0 (TID 41) in 41 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,017 INFO cluster.YarnScheduler: Removed TaskSet 53.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,017 INFO scheduler.DAGScheduler: ResultStage 53 (collect at AnalysisRunner.scala:326) finished in 0.048 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,017 INFO scheduler.DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,017 INFO cluster.YarnScheduler: Killing all running tasks in stage 53: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,018 INFO scheduler.DAGScheduler: Job 36 finished: collect at AnalysisRunner.scala:326, took 0.055640 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,066 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,066 INFO scheduler.DAGScheduler: Registering RDD 220 (countByKey at ColumnProfiler.scala:592) as input to shuffle 17\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,066 INFO scheduler.DAGScheduler: Got job 37 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,067 INFO scheduler.DAGScheduler: Final stage: ResultStage 55 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,067 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 54)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,067 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 54)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,067 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 54 (MapPartitionsRDD[220] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,072 INFO memory.MemoryStore: Block broadcast_44 stored as values in memory (estimated size 40.5 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,073 INFO memory.MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,073 INFO storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on 10.0.143.102:45637 (size: 17.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,073 INFO spark.SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,074 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 54 (MapPartitionsRDD[220] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,074 INFO cluster.YarnScheduler: Adding task set 54.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,075 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 54.0 (TID 42) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,083 INFO storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on algo-1:40231 (size: 17.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,104 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 54.0 (TID 42) in 29 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,104 INFO cluster.YarnScheduler: Removed TaskSet 54.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,105 INFO scheduler.DAGScheduler: ShuffleMapStage 54 (countByKey at ColumnProfiler.scala:592) finished in 0.037 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,105 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,105 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,105 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 55)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,105 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,105 INFO scheduler.DAGScheduler: Submitting ResultStage 55 (ShuffledRDD[221] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,106 INFO memory.MemoryStore: Block broadcast_45 stored as values in memory (estimated size 5.1 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,107 INFO memory.MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,108 INFO storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on 10.0.143.102:45637 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,108 INFO spark.SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,108 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 55 (ShuffledRDD[221] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,108 INFO cluster.YarnScheduler: Adding task set 55.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,109 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 55.0 (TID 43) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,116 INFO storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on algo-1:40231 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,118 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 17 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,126 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 55.0 (TID 43) in 17 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,126 INFO cluster.YarnScheduler: Removed TaskSet 55.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,127 INFO scheduler.DAGScheduler: ResultStage 55 (countByKey at ColumnProfiler.scala:592) finished in 0.021 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,127 INFO scheduler.DAGScheduler: Job 37 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,127 INFO cluster.YarnScheduler: Killing all running tasks in stage 55: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,128 INFO scheduler.DAGScheduler: Job 37 finished: countByKey at ColumnProfiler.scala:592, took 0.061706 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,240 INFO scheduler.DAGScheduler: Registering RDD 226 (collect at AnalysisRunner.scala:326) as input to shuffle 18\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,241 INFO scheduler.DAGScheduler: Got map stage job 38 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,241 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 56 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,241 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,242 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,242 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 56 (MapPartitionsRDD[226] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,245 INFO memory.MemoryStore: Block broadcast_46 stored as values in memory (estimated size 93.5 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,247 INFO memory.MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 30.0 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,247 INFO storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on 10.0.143.102:45637 (size: 30.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,248 INFO spark.SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,248 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 56 (MapPartitionsRDD[226] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,248 INFO cluster.YarnScheduler: Adding task set 56.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,249 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 56.0 (TID 44) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,258 INFO storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on algo-1:40231 (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,371 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 56.0 (TID 44) in 121 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,371 INFO cluster.YarnScheduler: Removed TaskSet 56.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,371 INFO scheduler.DAGScheduler: ShuffleMapStage 56 (collect at AnalysisRunner.scala:326) finished in 0.128 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,372 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,372 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,372 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,372 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,406 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,407 INFO scheduler.DAGScheduler: Got job 39 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,407 INFO scheduler.DAGScheduler: Final stage: ResultStage 58 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,407 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 57)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,407 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,407 INFO scheduler.DAGScheduler: Submitting ResultStage 58 (MapPartitionsRDD[229] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,417 INFO memory.MemoryStore: Block broadcast_47 stored as values in memory (estimated size 179.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,419 INFO memory.MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 49.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,421 INFO storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on 10.0.143.102:45637 (size: 49.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,421 INFO spark.SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,422 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 58 (MapPartitionsRDD[229] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,422 INFO cluster.YarnScheduler: Adding task set 58.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,423 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 58.0 (TID 45) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,433 INFO storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on algo-1:40231 (size: 49.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,441 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 18 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,520 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 58.0 (TID 45) in 97 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,521 INFO cluster.YarnScheduler: Removed TaskSet 58.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,521 INFO scheduler.DAGScheduler: ResultStage 58 (collect at AnalysisRunner.scala:326) finished in 0.113 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,522 INFO scheduler.DAGScheduler: Job 39 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,523 INFO cluster.YarnScheduler: Killing all running tasks in stage 58: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,523 INFO scheduler.DAGScheduler: Job 39 finished: collect at AnalysisRunner.scala:326, took 0.117015 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,603 INFO codegen.CodeGenerator: Code generated in 16.218165 ms\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,630 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,630 INFO scheduler.DAGScheduler: Got job 40 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,631 INFO scheduler.DAGScheduler: Final stage: ResultStage 59 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,631 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,631 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,631 INFO scheduler.DAGScheduler: Submitting ResultStage 59 (MapPartitionsRDD[239] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,636 INFO memory.MemoryStore: Block broadcast_48 stored as values in memory (estimated size 48.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,638 INFO memory.MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 19.3 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,639 INFO storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on 10.0.143.102:45637 (size: 19.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,639 INFO spark.SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,640 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 59 (MapPartitionsRDD[239] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,640 INFO cluster.YarnScheduler: Adding task set 59.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,641 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 59.0 (TID 46) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,651 INFO storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on algo-1:40231 (size: 19.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,762 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 59.0 (TID 46) in 121 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,762 INFO cluster.YarnScheduler: Removed TaskSet 59.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,763 INFO scheduler.DAGScheduler: ResultStage 59 (treeReduce at KLLRunner.scala:107) finished in 0.131 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,766 INFO scheduler.DAGScheduler: Job 40 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,767 INFO cluster.YarnScheduler: Killing all running tasks in stage 59: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,767 INFO scheduler.DAGScheduler: Job 40 finished: treeReduce at KLLRunner.scala:107, took 0.137472 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,907 INFO codegen.CodeGenerator: Code generated in 33.293419 ms\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,912 INFO scheduler.DAGScheduler: Registering RDD 244 (collect at AnalysisRunner.scala:326) as input to shuffle 19\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,913 INFO scheduler.DAGScheduler: Got map stage job 41 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,913 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 60 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,913 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,913 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,914 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 60 (MapPartitionsRDD[244] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,917 INFO memory.MemoryStore: Block broadcast_49 stored as values in memory (estimated size 83.8 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,919 INFO memory.MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 26.2 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,919 INFO storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on 10.0.143.102:45637 (size: 26.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,919 INFO spark.SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,920 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 60 (MapPartitionsRDD[244] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,920 INFO cluster.YarnScheduler: Adding task set 60.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,921 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 60.0 (TID 47) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:46,931 INFO storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on algo-1:40231 (size: 26.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,068 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 60.0 (TID 47) in 147 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,068 INFO cluster.YarnScheduler: Removed TaskSet 60.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,069 INFO scheduler.DAGScheduler: ShuffleMapStage 60 (collect at AnalysisRunner.scala:326) finished in 0.155 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,069 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,069 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,069 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,069 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,119 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,120 INFO scheduler.DAGScheduler: Got job 42 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,120 INFO scheduler.DAGScheduler: Final stage: ResultStage 62 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,120 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 61)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,120 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,120 INFO scheduler.DAGScheduler: Submitting ResultStage 62 (MapPartitionsRDD[247] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,122 INFO memory.MemoryStore: Block broadcast_50 stored as values in memory (estimated size 66.2 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,123 INFO memory.MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,124 INFO storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on 10.0.143.102:45637 (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,124 INFO spark.SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,124 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 62 (MapPartitionsRDD[247] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,124 INFO cluster.YarnScheduler: Adding task set 62.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,125 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 62.0 (TID 48) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,137 INFO storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on algo-1:40231 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,141 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 19 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,146 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 62.0 (TID 48) in 21 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,146 INFO cluster.YarnScheduler: Removed TaskSet 62.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,147 INFO scheduler.DAGScheduler: ResultStage 62 (collect at AnalysisRunner.scala:326) finished in 0.026 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,148 INFO scheduler.DAGScheduler: Job 42 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,148 INFO cluster.YarnScheduler: Killing all running tasks in stage 62: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,148 INFO scheduler.DAGScheduler: Job 42 finished: collect at AnalysisRunner.scala:326, took 0.028194 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,205 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,206 INFO scheduler.DAGScheduler: Registering RDD 255 (countByKey at ColumnProfiler.scala:592) as input to shuffle 20\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,206 INFO scheduler.DAGScheduler: Got job 43 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,206 INFO scheduler.DAGScheduler: Final stage: ResultStage 64 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,206 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 63)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,206 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 63)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,208 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 63 (MapPartitionsRDD[255] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,213 INFO memory.MemoryStore: Block broadcast_51 stored as values in memory (estimated size 40.5 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,216 INFO memory.MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 17.2 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,216 INFO storage.BlockManagerInfo: Added broadcast_51_piece0 in memory on 10.0.143.102:45637 (size: 17.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,216 INFO spark.SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,217 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 63 (MapPartitionsRDD[255] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,217 INFO cluster.YarnScheduler: Adding task set 63.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,218 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 63.0 (TID 49) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,224 INFO storage.BlockManagerInfo: Added broadcast_51_piece0 in memory on algo-1:40231 (size: 17.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,250 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 63.0 (TID 49) in 33 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,250 INFO cluster.YarnScheduler: Removed TaskSet 63.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,251 INFO scheduler.DAGScheduler: ShuffleMapStage 63 (countByKey at ColumnProfiler.scala:592) finished in 0.043 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,251 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,251 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,251 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 64)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,251 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,251 INFO scheduler.DAGScheduler: Submitting ResultStage 64 (ShuffledRDD[256] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,253 INFO memory.MemoryStore: Block broadcast_52 stored as values in memory (estimated size 5.1 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,254 INFO memory.MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,254 INFO storage.BlockManagerInfo: Added broadcast_52_piece0 in memory on 10.0.143.102:45637 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,254 INFO spark.SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,255 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 64 (ShuffledRDD[256] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,255 INFO cluster.YarnScheduler: Adding task set 64.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,256 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 64.0 (TID 50) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,262 INFO storage.BlockManagerInfo: Added broadcast_52_piece0 in memory on algo-1:40231 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,264 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 20 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,271 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 64.0 (TID 50) in 16 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,271 INFO cluster.YarnScheduler: Removed TaskSet 64.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,272 INFO scheduler.DAGScheduler: ResultStage 64 (countByKey at ColumnProfiler.scala:592) finished in 0.019 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,272 INFO scheduler.DAGScheduler: Job 43 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,272 INFO cluster.YarnScheduler: Killing all running tasks in stage 64: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,272 INFO scheduler.DAGScheduler: Job 43 finished: countByKey at ColumnProfiler.scala:592, took 0.066777 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,377 INFO scheduler.DAGScheduler: Registering RDD 261 (collect at AnalysisRunner.scala:326) as input to shuffle 21\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,379 INFO scheduler.DAGScheduler: Got map stage job 44 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,379 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 65 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,380 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,380 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,380 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 65 (MapPartitionsRDD[261] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,385 INFO memory.MemoryStore: Block broadcast_53 stored as values in memory (estimated size 93.5 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,389 INFO memory.MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 30.0 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,389 INFO storage.BlockManagerInfo: Added broadcast_53_piece0 in memory on 10.0.143.102:45637 (size: 30.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,390 INFO spark.SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,390 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 65 (MapPartitionsRDD[261] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,391 INFO cluster.YarnScheduler: Adding task set 65.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,392 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 65.0 (TID 51) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,401 INFO storage.BlockManagerInfo: Added broadcast_53_piece0 in memory on algo-1:40231 (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,517 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 65.0 (TID 51) in 124 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,517 INFO cluster.YarnScheduler: Removed TaskSet 65.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,517 INFO scheduler.DAGScheduler: ShuffleMapStage 65 (collect at AnalysisRunner.scala:326) finished in 0.135 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,517 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,517 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,517 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,517 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,549 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,550 INFO scheduler.DAGScheduler: Got job 45 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,550 INFO scheduler.DAGScheduler: Final stage: ResultStage 67 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,550 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 66)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,550 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,550 INFO scheduler.DAGScheduler: Submitting ResultStage 67 (MapPartitionsRDD[264] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,556 INFO memory.MemoryStore: Block broadcast_54 stored as values in memory (estimated size 179.2 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,557 INFO memory.MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 49.6 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,558 INFO storage.BlockManagerInfo: Added broadcast_54_piece0 in memory on 10.0.143.102:45637 (size: 49.6 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,558 INFO spark.SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,558 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 67 (MapPartitionsRDD[264] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,559 INFO cluster.YarnScheduler: Adding task set 67.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,560 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 67.0 (TID 52) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,568 INFO storage.BlockManagerInfo: Added broadcast_54_piece0 in memory on algo-1:40231 (size: 49.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,578 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 21 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,655 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 67.0 (TID 52) in 95 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,655 INFO cluster.YarnScheduler: Removed TaskSet 67.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,656 INFO scheduler.DAGScheduler: ResultStage 67 (collect at AnalysisRunner.scala:326) finished in 0.105 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,656 INFO scheduler.DAGScheduler: Job 45 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,662 INFO cluster.YarnScheduler: Killing all running tasks in stage 67: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,662 INFO scheduler.DAGScheduler: Job 45 finished: collect at AnalysisRunner.scala:326, took 0.113428 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,745 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,746 INFO scheduler.DAGScheduler: Got job 46 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,746 INFO scheduler.DAGScheduler: Final stage: ResultStage 68 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,746 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,747 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,751 INFO scheduler.DAGScheduler: Submitting ResultStage 68 (MapPartitionsRDD[274] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,760 INFO memory.MemoryStore: Block broadcast_55 stored as values in memory (estimated size 48.1 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,761 INFO memory.MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,762 INFO storage.BlockManagerInfo: Added broadcast_55_piece0 in memory on 10.0.143.102:45637 (size: 19.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,762 INFO spark.SparkContext: Created broadcast 55 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,762 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 68 (MapPartitionsRDD[274] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,762 INFO cluster.YarnScheduler: Adding task set 68.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,764 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 68.0 (TID 53) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,773 INFO storage.BlockManagerInfo: Added broadcast_55_piece0 in memory on algo-1:40231 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,828 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 68.0 (TID 53) in 65 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,828 INFO cluster.YarnScheduler: Removed TaskSet 68.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,829 INFO scheduler.DAGScheduler: ResultStage 68 (treeReduce at KLLRunner.scala:107) finished in 0.078 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,829 INFO scheduler.DAGScheduler: Job 46 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,830 INFO cluster.YarnScheduler: Killing all running tasks in stage 68: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,830 INFO scheduler.DAGScheduler: Job 46 finished: treeReduce at KLLRunner.scala:107, took 0.084476 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,979 INFO scheduler.DAGScheduler: Registering RDD 279 (collect at AnalysisRunner.scala:326) as input to shuffle 22\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,979 INFO scheduler.DAGScheduler: Got map stage job 47 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,979 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 69 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,980 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,980 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,981 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 69 (MapPartitionsRDD[279] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,991 INFO storage.BlockManagerInfo: Removed broadcast_55_piece0 on 10.0.143.102:45637 in memory (size: 19.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,991 INFO storage.BlockManagerInfo: Removed broadcast_55_piece0 on algo-1:40231 in memory (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,995 INFO memory.MemoryStore: Block broadcast_56 stored as values in memory (estimated size 83.8 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,996 INFO storage.BlockManagerInfo: Removed broadcast_50_piece0 on 10.0.143.102:45637 in memory (size: 19.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:47,997 INFO memory.MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 26.2 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,000 INFO storage.BlockManagerInfo: Added broadcast_56_piece0 in memory on 10.0.143.102:45637 (size: 26.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,000 INFO spark.SparkContext: Created broadcast 56 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,001 INFO storage.BlockManagerInfo: Removed broadcast_50_piece0 on algo-1:40231 in memory (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,002 INFO storage.BlockManagerInfo: Removed broadcast_51_piece0 on 10.0.143.102:45637 in memory (size: 17.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,003 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 69 (MapPartitionsRDD[279] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,003 INFO cluster.YarnScheduler: Adding task set 69.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,004 INFO storage.BlockManagerInfo: Removed broadcast_51_piece0 on algo-1:40231 in memory (size: 17.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,006 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 69.0 (TID 54) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,012 INFO storage.BlockManagerInfo: Removed broadcast_43_piece0 on 10.0.143.102:45637 in memory (size: 19.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,017 INFO storage.BlockManagerInfo: Added broadcast_56_piece0 in memory on algo-1:40231 (size: 26.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,017 INFO storage.BlockManagerInfo: Removed broadcast_43_piece0 on algo-1:40231 in memory (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,029 INFO storage.BlockManagerInfo: Removed broadcast_44_piece0 on 10.0.143.102:45637 in memory (size: 17.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,030 INFO storage.BlockManagerInfo: Removed broadcast_44_piece0 on algo-1:40231 in memory (size: 17.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,034 INFO storage.BlockManagerInfo: Removed broadcast_46_piece0 on 10.0.143.102:45637 in memory (size: 30.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,036 INFO storage.BlockManagerInfo: Removed broadcast_46_piece0 on algo-1:40231 in memory (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,038 INFO storage.BlockManagerInfo: Removed broadcast_52_piece0 on 10.0.143.102:45637 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,040 INFO storage.BlockManagerInfo: Removed broadcast_52_piece0 on algo-1:40231 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,042 INFO storage.BlockManagerInfo: Removed broadcast_47_piece0 on 10.0.143.102:45637 in memory (size: 49.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,043 INFO storage.BlockManagerInfo: Removed broadcast_47_piece0 on algo-1:40231 in memory (size: 49.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,045 INFO storage.BlockManagerInfo: Removed broadcast_45_piece0 on 10.0.143.102:45637 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,046 INFO storage.BlockManagerInfo: Removed broadcast_45_piece0 on algo-1:40231 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,047 INFO storage.BlockManagerInfo: Removed broadcast_53_piece0 on 10.0.143.102:45637 in memory (size: 30.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,049 INFO storage.BlockManagerInfo: Removed broadcast_53_piece0 on algo-1:40231 in memory (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,051 INFO storage.BlockManagerInfo: Removed broadcast_48_piece0 on 10.0.143.102:45637 in memory (size: 19.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,052 INFO storage.BlockManagerInfo: Removed broadcast_48_piece0 on algo-1:40231 in memory (size: 19.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,062 INFO storage.BlockManagerInfo: Removed broadcast_49_piece0 on 10.0.143.102:45637 in memory (size: 26.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,065 INFO storage.BlockManagerInfo: Removed broadcast_49_piece0 on algo-1:40231 in memory (size: 26.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,067 INFO storage.BlockManagerInfo: Removed broadcast_54_piece0 on 10.0.143.102:45637 in memory (size: 49.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,068 INFO storage.BlockManagerInfo: Removed broadcast_54_piece0 on algo-1:40231 in memory (size: 49.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,080 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 69.0 (TID 54) in 74 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,081 INFO cluster.YarnScheduler: Removed TaskSet 69.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,081 INFO scheduler.DAGScheduler: ShuffleMapStage 69 (collect at AnalysisRunner.scala:326) finished in 0.100 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,081 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,081 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,081 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,081 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,143 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,143 INFO scheduler.DAGScheduler: Got job 48 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,143 INFO scheduler.DAGScheduler: Final stage: ResultStage 71 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,143 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 70)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,144 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,144 INFO scheduler.DAGScheduler: Submitting ResultStage 71 (MapPartitionsRDD[282] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,146 INFO memory.MemoryStore: Block broadcast_57 stored as values in memory (estimated size 66.2 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,149 INFO memory.MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,150 INFO storage.BlockManagerInfo: Added broadcast_57_piece0 in memory on 10.0.143.102:45637 (size: 19.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,151 INFO spark.SparkContext: Created broadcast 57 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,151 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 71 (MapPartitionsRDD[282] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,151 INFO cluster.YarnScheduler: Adding task set 71.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,153 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 71.0 (TID 55) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,164 INFO storage.BlockManagerInfo: Added broadcast_57_piece0 in memory on algo-1:40231 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,168 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 22 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,177 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 71.0 (TID 55) in 24 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,177 INFO cluster.YarnScheduler: Removed TaskSet 71.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,177 INFO scheduler.DAGScheduler: ResultStage 71 (collect at AnalysisRunner.scala:326) finished in 0.032 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,177 INFO scheduler.DAGScheduler: Job 48 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,177 INFO cluster.YarnScheduler: Killing all running tasks in stage 71: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,178 INFO scheduler.DAGScheduler: Job 48 finished: collect at AnalysisRunner.scala:326, took 0.034899 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,234 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,234 INFO scheduler.DAGScheduler: Registering RDD 290 (countByKey at ColumnProfiler.scala:592) as input to shuffle 23\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,235 INFO scheduler.DAGScheduler: Got job 49 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,235 INFO scheduler.DAGScheduler: Final stage: ResultStage 73 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,235 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 72)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,236 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 72)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,237 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 72 (MapPartitionsRDD[290] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,244 INFO memory.MemoryStore: Block broadcast_58 stored as values in memory (estimated size 40.5 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,246 INFO memory.MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 17.2 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,246 INFO storage.BlockManagerInfo: Added broadcast_58_piece0 in memory on 10.0.143.102:45637 (size: 17.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,247 INFO spark.SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,247 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 72 (MapPartitionsRDD[290] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,248 INFO cluster.YarnScheduler: Adding task set 72.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,249 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 72.0 (TID 56) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,261 INFO storage.BlockManagerInfo: Added broadcast_58_piece0 in memory on algo-1:40231 (size: 17.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,306 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 72.0 (TID 56) in 57 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,306 INFO cluster.YarnScheduler: Removed TaskSet 72.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,307 INFO scheduler.DAGScheduler: ShuffleMapStage 72 (countByKey at ColumnProfiler.scala:592) finished in 0.069 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,307 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,307 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,308 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 73)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,309 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,309 INFO scheduler.DAGScheduler: Submitting ResultStage 73 (ShuffledRDD[291] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,310 INFO memory.MemoryStore: Block broadcast_59 stored as values in memory (estimated size 5.1 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,312 INFO memory.MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,313 INFO storage.BlockManagerInfo: Added broadcast_59_piece0 in memory on 10.0.143.102:45637 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,314 INFO spark.SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,315 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 73 (ShuffledRDD[291] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,315 INFO cluster.YarnScheduler: Adding task set 73.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,316 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 73.0 (TID 57) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,383 INFO storage.BlockManagerInfo: Added broadcast_59_piece0 in memory on algo-1:40231 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,389 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 23 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,412 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 73.0 (TID 57) in 96 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,412 INFO cluster.YarnScheduler: Removed TaskSet 73.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,413 INFO scheduler.DAGScheduler: ResultStage 73 (countByKey at ColumnProfiler.scala:592) finished in 0.103 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,414 INFO scheduler.DAGScheduler: Job 49 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,414 INFO cluster.YarnScheduler: Killing all running tasks in stage 73: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,415 INFO scheduler.DAGScheduler: Job 49 finished: countByKey at ColumnProfiler.scala:592, took 0.180982 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,546 INFO scheduler.DAGScheduler: Registering RDD 296 (collect at AnalysisRunner.scala:326) as input to shuffle 24\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,547 INFO scheduler.DAGScheduler: Got map stage job 50 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,547 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 74 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,547 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,548 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,549 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 74 (MapPartitionsRDD[296] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,554 INFO memory.MemoryStore: Block broadcast_60 stored as values in memory (estimated size 93.5 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,557 INFO memory.MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 30.0 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,557 INFO storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on 10.0.143.102:45637 (size: 30.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,558 INFO spark.SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,558 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 74 (MapPartitionsRDD[296] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,558 INFO cluster.YarnScheduler: Adding task set 74.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,559 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 74.0 (TID 58) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,571 INFO storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on algo-1:40231 (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,708 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 74.0 (TID 58) in 149 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,708 INFO cluster.YarnScheduler: Removed TaskSet 74.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,709 INFO scheduler.DAGScheduler: ShuffleMapStage 74 (collect at AnalysisRunner.scala:326) finished in 0.158 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,712 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,712 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,712 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,712 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,748 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,749 INFO scheduler.DAGScheduler: Got job 51 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,749 INFO scheduler.DAGScheduler: Final stage: ResultStage 76 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,749 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 75)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,749 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,749 INFO scheduler.DAGScheduler: Submitting ResultStage 76 (MapPartitionsRDD[299] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,756 INFO memory.MemoryStore: Block broadcast_61 stored as values in memory (estimated size 179.0 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,757 INFO memory.MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 49.5 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,757 INFO storage.BlockManagerInfo: Added broadcast_61_piece0 in memory on 10.0.143.102:45637 (size: 49.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,758 INFO spark.SparkContext: Created broadcast 61 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,758 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 76 (MapPartitionsRDD[299] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,758 INFO cluster.YarnScheduler: Adding task set 76.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,759 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 76.0 (TID 59) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,768 INFO storage.BlockManagerInfo: Added broadcast_61_piece0 in memory on algo-1:40231 (size: 49.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,777 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 24 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,853 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 76.0 (TID 59) in 94 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,853 INFO cluster.YarnScheduler: Removed TaskSet 76.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,854 INFO scheduler.DAGScheduler: ResultStage 76 (collect at AnalysisRunner.scala:326) finished in 0.104 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,854 INFO scheduler.DAGScheduler: Job 51 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,856 INFO cluster.YarnScheduler: Killing all running tasks in stage 76: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,857 INFO scheduler.DAGScheduler: Job 51 finished: collect at AnalysisRunner.scala:326, took 0.108476 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,939 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,940 INFO scheduler.DAGScheduler: Got job 52 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,940 INFO scheduler.DAGScheduler: Final stage: ResultStage 77 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,940 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,941 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,942 INFO scheduler.DAGScheduler: Submitting ResultStage 77 (MapPartitionsRDD[309] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,950 INFO memory.MemoryStore: Block broadcast_62 stored as values in memory (estimated size 48.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,951 INFO memory.MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 19.3 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,952 INFO storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on 10.0.143.102:45637 (size: 19.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,952 INFO spark.SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,952 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 77 (MapPartitionsRDD[309] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,952 INFO cluster.YarnScheduler: Adding task set 77.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,954 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 77.0 (TID 60) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:48,966 INFO storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on algo-1:40231 (size: 19.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,062 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 77.0 (TID 60) in 108 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,062 INFO cluster.YarnScheduler: Removed TaskSet 77.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,062 INFO scheduler.DAGScheduler: ResultStage 77 (treeReduce at KLLRunner.scala:107) finished in 0.119 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,063 INFO scheduler.DAGScheduler: Job 52 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,063 INFO cluster.YarnScheduler: Killing all running tasks in stage 77: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,064 INFO scheduler.DAGScheduler: Job 52 finished: treeReduce at KLLRunner.scala:107, took 0.123899 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,194 INFO scheduler.DAGScheduler: Registering RDD 314 (collect at AnalysisRunner.scala:326) as input to shuffle 25\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,194 INFO scheduler.DAGScheduler: Got map stage job 53 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,194 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 78 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,194 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,194 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,194 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 78 (MapPartitionsRDD[314] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,197 INFO memory.MemoryStore: Block broadcast_63 stored as values in memory (estimated size 83.8 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,198 INFO memory.MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 26.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,199 INFO storage.BlockManagerInfo: Added broadcast_63_piece0 in memory on 10.0.143.102:45637 (size: 26.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,199 INFO spark.SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,200 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 78 (MapPartitionsRDD[314] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,200 INFO cluster.YarnScheduler: Adding task set 78.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,202 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 78.0 (TID 61) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,211 INFO storage.BlockManagerInfo: Added broadcast_63_piece0 in memory on algo-1:40231 (size: 26.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,240 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 78.0 (TID 61) in 38 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,241 INFO cluster.YarnScheduler: Removed TaskSet 78.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,242 INFO scheduler.DAGScheduler: ShuffleMapStage 78 (collect at AnalysisRunner.scala:326) finished in 0.047 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,243 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,243 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,243 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,244 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,285 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,286 INFO scheduler.DAGScheduler: Got job 54 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,286 INFO scheduler.DAGScheduler: Final stage: ResultStage 80 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,286 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 79)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,287 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,287 INFO scheduler.DAGScheduler: Submitting ResultStage 80 (MapPartitionsRDD[317] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,288 INFO memory.MemoryStore: Block broadcast_64 stored as values in memory (estimated size 66.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,290 INFO memory.MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,290 INFO storage.BlockManagerInfo: Added broadcast_64_piece0 in memory on 10.0.143.102:45637 (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,291 INFO spark.SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,291 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 80 (MapPartitionsRDD[317] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,291 INFO cluster.YarnScheduler: Adding task set 80.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,292 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 80.0 (TID 62) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,298 INFO storage.BlockManagerInfo: Added broadcast_64_piece0 in memory on algo-1:40231 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,302 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 25 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,308 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 80.0 (TID 62) in 16 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,308 INFO cluster.YarnScheduler: Removed TaskSet 80.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,309 INFO scheduler.DAGScheduler: ResultStage 80 (collect at AnalysisRunner.scala:326) finished in 0.021 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,309 INFO scheduler.DAGScheduler: Job 54 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,309 INFO cluster.YarnScheduler: Killing all running tasks in stage 80: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,309 INFO scheduler.DAGScheduler: Job 54 finished: collect at AnalysisRunner.scala:326, took 0.023654 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,368 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,369 INFO scheduler.DAGScheduler: Registering RDD 325 (countByKey at ColumnProfiler.scala:592) as input to shuffle 26\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,369 INFO scheduler.DAGScheduler: Got job 55 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,370 INFO scheduler.DAGScheduler: Final stage: ResultStage 82 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,370 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 81)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,370 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 81)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,370 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 81 (MapPartitionsRDD[325] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,374 INFO memory.MemoryStore: Block broadcast_65 stored as values in memory (estimated size 40.5 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,377 INFO memory.MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,379 INFO storage.BlockManagerInfo: Added broadcast_65_piece0 in memory on 10.0.143.102:45637 (size: 17.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,380 INFO spark.SparkContext: Created broadcast 65 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,380 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 81 (MapPartitionsRDD[325] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,382 INFO cluster.YarnScheduler: Adding task set 81.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,385 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 81.0 (TID 63) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,392 INFO storage.BlockManagerInfo: Added broadcast_65_piece0 in memory on algo-1:40231 (size: 17.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,449 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 81.0 (TID 63) in 64 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,449 INFO cluster.YarnScheduler: Removed TaskSet 81.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,450 INFO scheduler.DAGScheduler: ShuffleMapStage 81 (countByKey at ColumnProfiler.scala:592) finished in 0.079 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,451 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,451 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,451 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 82)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,451 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,452 INFO scheduler.DAGScheduler: Submitting ResultStage 82 (ShuffledRDD[326] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,453 INFO memory.MemoryStore: Block broadcast_66 stored as values in memory (estimated size 5.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,454 INFO memory.MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,455 INFO storage.BlockManagerInfo: Added broadcast_66_piece0 in memory on 10.0.143.102:45637 (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,455 INFO spark.SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,456 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 82 (ShuffledRDD[326] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,456 INFO cluster.YarnScheduler: Adding task set 82.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,457 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 82.0 (TID 64) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,468 INFO storage.BlockManagerInfo: Added broadcast_66_piece0 in memory on algo-1:40231 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,471 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 26 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,490 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 82.0 (TID 64) in 33 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,490 INFO cluster.YarnScheduler: Removed TaskSet 82.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,491 INFO scheduler.DAGScheduler: ResultStage 82 (countByKey at ColumnProfiler.scala:592) finished in 0.039 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,491 INFO scheduler.DAGScheduler: Job 55 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,492 INFO cluster.YarnScheduler: Killing all running tasks in stage 82: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,492 INFO scheduler.DAGScheduler: Job 55 finished: countByKey at ColumnProfiler.scala:592, took 0.123574 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,562 INFO scheduler.DAGScheduler: Registering RDD 331 (collect at AnalysisRunner.scala:326) as input to shuffle 27\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,563 INFO scheduler.DAGScheduler: Got map stage job 56 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,563 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 83 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,563 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,563 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,563 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 83 (MapPartitionsRDD[331] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,569 INFO memory.MemoryStore: Block broadcast_67 stored as values in memory (estimated size 93.5 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,570 INFO memory.MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 30.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,570 INFO storage.BlockManagerInfo: Added broadcast_67_piece0 in memory on 10.0.143.102:45637 (size: 30.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,571 INFO spark.SparkContext: Created broadcast 67 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,571 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 83 (MapPartitionsRDD[331] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,571 INFO cluster.YarnScheduler: Adding task set 83.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,572 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 83.0 (TID 65) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,585 INFO storage.BlockManagerInfo: Added broadcast_67_piece0 in memory on algo-1:40231 (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,714 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 83.0 (TID 65) in 142 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,714 INFO cluster.YarnScheduler: Removed TaskSet 83.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,715 INFO scheduler.DAGScheduler: ShuffleMapStage 83 (collect at AnalysisRunner.scala:326) finished in 0.150 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,715 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,716 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,716 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,716 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,756 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,757 INFO scheduler.DAGScheduler: Got job 57 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,757 INFO scheduler.DAGScheduler: Final stage: ResultStage 85 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,758 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 84)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,758 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,758 INFO scheduler.DAGScheduler: Submitting ResultStage 85 (MapPartitionsRDD[334] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,769 INFO memory.MemoryStore: Block broadcast_68 stored as values in memory (estimated size 178.8 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,770 INFO memory.MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 49.4 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,771 INFO storage.BlockManagerInfo: Added broadcast_68_piece0 in memory on 10.0.143.102:45637 (size: 49.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,772 INFO spark.SparkContext: Created broadcast 68 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,773 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 85 (MapPartitionsRDD[334] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,773 INFO cluster.YarnScheduler: Adding task set 85.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,774 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 85.0 (TID 66) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,787 INFO storage.BlockManagerInfo: Added broadcast_68_piece0 in memory on algo-1:40231 (size: 49.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,798 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 27 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,942 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 85.0 (TID 66) in 168 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,942 INFO cluster.YarnScheduler: Removed TaskSet 85.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,943 INFO scheduler.DAGScheduler: ResultStage 85 (collect at AnalysisRunner.scala:326) finished in 0.184 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,943 INFO scheduler.DAGScheduler: Job 57 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,944 INFO cluster.YarnScheduler: Killing all running tasks in stage 85: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:49,944 INFO scheduler.DAGScheduler: Job 57 finished: collect at AnalysisRunner.scala:326, took 0.187380 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,007 INFO codegen.CodeGenerator: Code generated in 10.665212 ms\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,033 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,034 INFO scheduler.DAGScheduler: Got job 58 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,034 INFO scheduler.DAGScheduler: Final stage: ResultStage 86 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,034 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,034 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,035 INFO scheduler.DAGScheduler: Submitting ResultStage 86 (MapPartitionsRDD[344] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,039 INFO memory.MemoryStore: Block broadcast_69 stored as values in memory (estimated size 48.1 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,040 INFO memory.MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 19.3 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,042 INFO storage.BlockManagerInfo: Added broadcast_69_piece0 in memory on 10.0.143.102:45637 (size: 19.3 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,044 INFO spark.SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,044 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 86 (MapPartitionsRDD[344] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,044 INFO cluster.YarnScheduler: Adding task set 86.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,045 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 86.0 (TID 67) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,051 INFO storage.BlockManagerInfo: Added broadcast_69_piece0 in memory on algo-1:40231 (size: 19.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,151 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 86.0 (TID 67) in 106 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,152 INFO cluster.YarnScheduler: Removed TaskSet 86.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,153 INFO scheduler.DAGScheduler: ResultStage 86 (treeReduce at KLLRunner.scala:107) finished in 0.118 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,155 INFO scheduler.DAGScheduler: Job 58 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,156 INFO cluster.YarnScheduler: Killing all running tasks in stage 86: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,156 INFO scheduler.DAGScheduler: Job 58 finished: treeReduce at KLLRunner.scala:107, took 0.122602 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,261 INFO storage.BlockManagerInfo: Removed broadcast_57_piece0 on 10.0.143.102:45637 in memory (size: 19.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,268 INFO storage.BlockManagerInfo: Removed broadcast_57_piece0 on algo-1:40231 in memory (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,272 INFO storage.BlockManagerInfo: Removed broadcast_67_piece0 on 10.0.143.102:45637 in memory (size: 30.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,277 INFO storage.BlockManagerInfo: Removed broadcast_67_piece0 on algo-1:40231 in memory (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,284 INFO storage.BlockManagerInfo: Removed broadcast_62_piece0 on 10.0.143.102:45637 in memory (size: 19.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,288 INFO storage.BlockManagerInfo: Removed broadcast_62_piece0 on algo-1:40231 in memory (size: 19.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,291 INFO storage.BlockManagerInfo: Removed broadcast_59_piece0 on 10.0.143.102:45637 in memory (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,292 INFO storage.BlockManagerInfo: Removed broadcast_59_piece0 on algo-1:40231 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,295 INFO storage.BlockManagerInfo: Removed broadcast_60_piece0 on 10.0.143.102:45637 in memory (size: 30.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,295 INFO storage.BlockManagerInfo: Removed broadcast_60_piece0 on algo-1:40231 in memory (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,298 INFO storage.BlockManagerInfo: Removed broadcast_58_piece0 on 10.0.143.102:45637 in memory (size: 17.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,299 INFO storage.BlockManagerInfo: Removed broadcast_58_piece0 on algo-1:40231 in memory (size: 17.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,301 INFO storage.BlockManagerInfo: Removed broadcast_69_piece0 on 10.0.143.102:45637 in memory (size: 19.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,302 INFO storage.BlockManagerInfo: Removed broadcast_69_piece0 on algo-1:40231 in memory (size: 19.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,305 INFO storage.BlockManagerInfo: Removed broadcast_65_piece0 on 10.0.143.102:45637 in memory (size: 17.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,305 INFO storage.BlockManagerInfo: Removed broadcast_65_piece0 on algo-1:40231 in memory (size: 17.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,314 INFO storage.BlockManagerInfo: Removed broadcast_63_piece0 on 10.0.143.102:45637 in memory (size: 26.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,315 INFO storage.BlockManagerInfo: Removed broadcast_63_piece0 on algo-1:40231 in memory (size: 26.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,317 INFO storage.BlockManagerInfo: Removed broadcast_61_piece0 on 10.0.143.102:45637 in memory (size: 49.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,317 INFO storage.BlockManagerInfo: Removed broadcast_61_piece0 on algo-1:40231 in memory (size: 49.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,319 INFO storage.BlockManagerInfo: Removed broadcast_68_piece0 on 10.0.143.102:45637 in memory (size: 49.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,319 INFO storage.BlockManagerInfo: Removed broadcast_68_piece0 on algo-1:40231 in memory (size: 49.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,321 INFO storage.BlockManagerInfo: Removed broadcast_66_piece0 on 10.0.143.102:45637 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,322 INFO storage.BlockManagerInfo: Removed broadcast_66_piece0 on algo-1:40231 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,323 INFO storage.BlockManagerInfo: Removed broadcast_56_piece0 on 10.0.143.102:45637 in memory (size: 26.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,324 INFO storage.BlockManagerInfo: Removed broadcast_56_piece0 on algo-1:40231 in memory (size: 26.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,325 INFO storage.BlockManagerInfo: Removed broadcast_64_piece0 on 10.0.143.102:45637 in memory (size: 19.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,326 INFO storage.BlockManagerInfo: Removed broadcast_64_piece0 on algo-1:40231 in memory (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,360 INFO codegen.CodeGenerator: Code generated in 49.0196 ms\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,364 INFO scheduler.DAGScheduler: Registering RDD 349 (collect at AnalysisRunner.scala:326) as input to shuffle 28\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,365 INFO scheduler.DAGScheduler: Got map stage job 59 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,365 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 87 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,365 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,365 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,365 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 87 (MapPartitionsRDD[349] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,368 INFO memory.MemoryStore: Block broadcast_70 stored as values in memory (estimated size 83.8 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,369 INFO memory.MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 26.2 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,370 INFO storage.BlockManagerInfo: Added broadcast_70_piece0 in memory on 10.0.143.102:45637 (size: 26.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,370 INFO spark.SparkContext: Created broadcast 70 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,371 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 87 (MapPartitionsRDD[349] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,371 INFO cluster.YarnScheduler: Adding task set 87.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,372 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 87.0 (TID 68) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,380 INFO storage.BlockManagerInfo: Added broadcast_70_piece0 in memory on algo-1:40231 (size: 26.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,445 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 87.0 (TID 68) in 74 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,445 INFO cluster.YarnScheduler: Removed TaskSet 87.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,446 INFO scheduler.DAGScheduler: ShuffleMapStage 87 (collect at AnalysisRunner.scala:326) finished in 0.080 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,446 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,447 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,447 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,447 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,485 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,486 INFO scheduler.DAGScheduler: Got job 60 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,486 INFO scheduler.DAGScheduler: Final stage: ResultStage 89 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,486 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 88)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,486 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,486 INFO scheduler.DAGScheduler: Submitting ResultStage 89 (MapPartitionsRDD[352] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,487 INFO memory.MemoryStore: Block broadcast_71 stored as values in memory (estimated size 66.2 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,488 INFO memory.MemoryStore: Block broadcast_71_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,489 INFO storage.BlockManagerInfo: Added broadcast_71_piece0 in memory on 10.0.143.102:45637 (size: 19.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,489 INFO spark.SparkContext: Created broadcast 71 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,489 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 89 (MapPartitionsRDD[352] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,489 INFO cluster.YarnScheduler: Adding task set 89.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,490 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 89.0 (TID 69) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,497 INFO storage.BlockManagerInfo: Added broadcast_71_piece0 in memory on algo-1:40231 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,501 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 28 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,509 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 89.0 (TID 69) in 19 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,509 INFO cluster.YarnScheduler: Removed TaskSet 89.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,510 INFO scheduler.DAGScheduler: ResultStage 89 (collect at AnalysisRunner.scala:326) finished in 0.024 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,510 INFO scheduler.DAGScheduler: Job 60 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,510 INFO cluster.YarnScheduler: Killing all running tasks in stage 89: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,516 INFO scheduler.DAGScheduler: Job 60 finished: collect at AnalysisRunner.scala:326, took 0.031119 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,577 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,578 INFO scheduler.DAGScheduler: Registering RDD 360 (countByKey at ColumnProfiler.scala:592) as input to shuffle 29\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,578 INFO scheduler.DAGScheduler: Got job 61 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,578 INFO scheduler.DAGScheduler: Final stage: ResultStage 91 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,578 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 90)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,578 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 90)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,579 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 90 (MapPartitionsRDD[360] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,585 INFO memory.MemoryStore: Block broadcast_72 stored as values in memory (estimated size 40.5 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,586 INFO memory.MemoryStore: Block broadcast_72_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,586 INFO storage.BlockManagerInfo: Added broadcast_72_piece0 in memory on 10.0.143.102:45637 (size: 17.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,586 INFO spark.SparkContext: Created broadcast 72 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,587 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 90 (MapPartitionsRDD[360] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,587 INFO cluster.YarnScheduler: Adding task set 90.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,588 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 90.0 (TID 70) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,595 INFO storage.BlockManagerInfo: Added broadcast_72_piece0 in memory on algo-1:40231 (size: 17.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,617 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 90.0 (TID 70) in 29 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,617 INFO cluster.YarnScheduler: Removed TaskSet 90.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,617 INFO scheduler.DAGScheduler: ShuffleMapStage 90 (countByKey at ColumnProfiler.scala:592) finished in 0.038 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,618 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,618 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,618 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 91)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,618 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,618 INFO scheduler.DAGScheduler: Submitting ResultStage 91 (ShuffledRDD[361] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,619 INFO memory.MemoryStore: Block broadcast_73 stored as values in memory (estimated size 5.1 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,620 INFO memory.MemoryStore: Block broadcast_73_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,620 INFO storage.BlockManagerInfo: Added broadcast_73_piece0 in memory on 10.0.143.102:45637 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,621 INFO spark.SparkContext: Created broadcast 73 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,622 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 91 (ShuffledRDD[361] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,622 INFO cluster.YarnScheduler: Adding task set 91.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,622 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 91.0 (TID 71) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,627 INFO storage.BlockManagerInfo: Added broadcast_73_piece0 in memory on algo-1:40231 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,629 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 29 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,635 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 91.0 (TID 71) in 13 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,635 INFO scheduler.DAGScheduler: ResultStage 91 (countByKey at ColumnProfiler.scala:592) finished in 0.016 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,636 INFO scheduler.DAGScheduler: Job 61 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,635 INFO cluster.YarnScheduler: Removed TaskSet 91.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,636 INFO cluster.YarnScheduler: Killing all running tasks in stage 91: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,636 INFO scheduler.DAGScheduler: Job 61 finished: countByKey at ColumnProfiler.scala:592, took 0.058639 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,777 INFO scheduler.DAGScheduler: Registering RDD 366 (collect at AnalysisRunner.scala:326) as input to shuffle 30\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,777 INFO scheduler.DAGScheduler: Got map stage job 62 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,777 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 92 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,778 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,778 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,778 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 92 (MapPartitionsRDD[366] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,782 INFO memory.MemoryStore: Block broadcast_74 stored as values in memory (estimated size 93.5 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,783 INFO memory.MemoryStore: Block broadcast_74_piece0 stored as bytes in memory (estimated size 30.1 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,783 INFO storage.BlockManagerInfo: Added broadcast_74_piece0 in memory on 10.0.143.102:45637 (size: 30.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,784 INFO spark.SparkContext: Created broadcast 74 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,784 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 92 (MapPartitionsRDD[366] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,784 INFO cluster.YarnScheduler: Adding task set 92.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,785 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 92.0 (TID 72) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,791 INFO storage.BlockManagerInfo: Added broadcast_74_piece0 in memory on algo-1:40231 (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,891 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 92.0 (TID 72) in 106 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,891 INFO cluster.YarnScheduler: Removed TaskSet 92.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,892 INFO scheduler.DAGScheduler: ShuffleMapStage 92 (collect at AnalysisRunner.scala:326) finished in 0.113 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,892 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,892 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,892 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,892 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,924 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,925 INFO scheduler.DAGScheduler: Got job 63 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,925 INFO scheduler.DAGScheduler: Final stage: ResultStage 94 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,925 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 93)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,925 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,925 INFO scheduler.DAGScheduler: Submitting ResultStage 94 (MapPartitionsRDD[369] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,932 INFO memory.MemoryStore: Block broadcast_75 stored as values in memory (estimated size 179.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,934 INFO memory.MemoryStore: Block broadcast_75_piece0 stored as bytes in memory (estimated size 49.5 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,934 INFO storage.BlockManagerInfo: Added broadcast_75_piece0 in memory on 10.0.143.102:45637 (size: 49.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,935 INFO spark.SparkContext: Created broadcast 75 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,935 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 94 (MapPartitionsRDD[369] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,935 INFO cluster.YarnScheduler: Adding task set 94.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,936 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 94.0 (TID 73) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,946 INFO storage.BlockManagerInfo: Added broadcast_75_piece0 in memory on algo-1:40231 (size: 49.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:50,955 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 30 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,025 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 94.0 (TID 73) in 89 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,025 INFO cluster.YarnScheduler: Removed TaskSet 94.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,026 INFO scheduler.DAGScheduler: ResultStage 94 (collect at AnalysisRunner.scala:326) finished in 0.100 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,026 INFO scheduler.DAGScheduler: Job 63 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,026 INFO cluster.YarnScheduler: Killing all running tasks in stage 94: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,027 INFO scheduler.DAGScheduler: Job 63 finished: collect at AnalysisRunner.scala:326, took 0.102085 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,107 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,107 INFO scheduler.DAGScheduler: Got job 64 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,107 INFO scheduler.DAGScheduler: Final stage: ResultStage 95 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,107 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,108 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,108 INFO scheduler.DAGScheduler: Submitting ResultStage 95 (MapPartitionsRDD[379] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,113 INFO memory.MemoryStore: Block broadcast_76 stored as values in memory (estimated size 48.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,114 INFO memory.MemoryStore: Block broadcast_76_piece0 stored as bytes in memory (estimated size 19.3 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,114 INFO storage.BlockManagerInfo: Added broadcast_76_piece0 in memory on 10.0.143.102:45637 (size: 19.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,115 INFO spark.SparkContext: Created broadcast 76 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,115 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 95 (MapPartitionsRDD[379] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,116 INFO cluster.YarnScheduler: Adding task set 95.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,117 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 95.0 (TID 74) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,123 INFO storage.BlockManagerInfo: Added broadcast_76_piece0 in memory on algo-1:40231 (size: 19.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,224 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 95.0 (TID 74) in 107 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,224 INFO scheduler.DAGScheduler: ResultStage 95 (treeReduce at KLLRunner.scala:107) finished in 0.115 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,225 INFO scheduler.DAGScheduler: Job 64 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,225 INFO cluster.YarnScheduler: Removed TaskSet 95.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,225 INFO cluster.YarnScheduler: Killing all running tasks in stage 95: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,226 INFO scheduler.DAGScheduler: Job 64 finished: treeReduce at KLLRunner.scala:107, took 0.118694 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,333 INFO scheduler.DAGScheduler: Registering RDD 384 (collect at AnalysisRunner.scala:326) as input to shuffle 31\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,333 INFO scheduler.DAGScheduler: Got map stage job 65 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,334 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 96 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,334 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,334 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,335 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 96 (MapPartitionsRDD[384] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,337 INFO memory.MemoryStore: Block broadcast_77 stored as values in memory (estimated size 83.8 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,338 INFO memory.MemoryStore: Block broadcast_77_piece0 stored as bytes in memory (estimated size 26.2 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,340 INFO storage.BlockManagerInfo: Added broadcast_77_piece0 in memory on 10.0.143.102:45637 (size: 26.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,340 INFO spark.SparkContext: Created broadcast 77 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,341 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 96 (MapPartitionsRDD[384] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,341 INFO cluster.YarnScheduler: Adding task set 96.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,342 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 96.0 (TID 75) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,349 INFO storage.BlockManagerInfo: Added broadcast_77_piece0 in memory on algo-1:40231 (size: 26.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,370 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 96.0 (TID 75) in 28 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,371 INFO cluster.YarnScheduler: Removed TaskSet 96.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,371 INFO scheduler.DAGScheduler: ShuffleMapStage 96 (collect at AnalysisRunner.scala:326) finished in 0.036 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,371 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,371 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,371 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,371 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,402 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,403 INFO scheduler.DAGScheduler: Got job 66 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,403 INFO scheduler.DAGScheduler: Final stage: ResultStage 98 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,403 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 97)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,403 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,403 INFO scheduler.DAGScheduler: Submitting ResultStage 98 (MapPartitionsRDD[387] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,405 INFO memory.MemoryStore: Block broadcast_78 stored as values in memory (estimated size 66.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,406 INFO memory.MemoryStore: Block broadcast_78_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,406 INFO storage.BlockManagerInfo: Added broadcast_78_piece0 in memory on 10.0.143.102:45637 (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,406 INFO spark.SparkContext: Created broadcast 78 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,406 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 98 (MapPartitionsRDD[387] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,407 INFO cluster.YarnScheduler: Adding task set 98.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,407 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 98.0 (TID 76) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,413 INFO storage.BlockManagerInfo: Added broadcast_78_piece0 in memory on algo-1:40231 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,417 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 31 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,420 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 98.0 (TID 76) in 13 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,420 INFO cluster.YarnScheduler: Removed TaskSet 98.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,420 INFO scheduler.DAGScheduler: ResultStage 98 (collect at AnalysisRunner.scala:326) finished in 0.016 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,421 INFO scheduler.DAGScheduler: Job 66 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,421 INFO cluster.YarnScheduler: Killing all running tasks in stage 98: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,421 INFO scheduler.DAGScheduler: Job 66 finished: collect at AnalysisRunner.scala:326, took 0.018354 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,462 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,463 INFO scheduler.DAGScheduler: Registering RDD 395 (countByKey at ColumnProfiler.scala:592) as input to shuffle 32\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,463 INFO scheduler.DAGScheduler: Got job 67 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,463 INFO scheduler.DAGScheduler: Final stage: ResultStage 100 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,463 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 99)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,463 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 99)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,463 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 99 (MapPartitionsRDD[395] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,467 INFO memory.MemoryStore: Block broadcast_79 stored as values in memory (estimated size 40.5 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,469 INFO memory.MemoryStore: Block broadcast_79_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,469 INFO storage.BlockManagerInfo: Added broadcast_79_piece0 in memory on 10.0.143.102:45637 (size: 17.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,469 INFO spark.SparkContext: Created broadcast 79 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,470 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 99 (MapPartitionsRDD[395] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,470 INFO cluster.YarnScheduler: Adding task set 99.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,470 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 99.0 (TID 77) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,479 INFO storage.BlockManagerInfo: Added broadcast_79_piece0 in memory on algo-1:40231 (size: 17.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,494 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 99.0 (TID 77) in 24 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,494 INFO cluster.YarnScheduler: Removed TaskSet 99.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,495 INFO scheduler.DAGScheduler: ShuffleMapStage 99 (countByKey at ColumnProfiler.scala:592) finished in 0.031 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,495 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,495 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,495 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 100)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,495 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,496 INFO scheduler.DAGScheduler: Submitting ResultStage 100 (ShuffledRDD[396] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,496 INFO memory.MemoryStore: Block broadcast_80 stored as values in memory (estimated size 5.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,497 INFO memory.MemoryStore: Block broadcast_80_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,498 INFO storage.BlockManagerInfo: Added broadcast_80_piece0 in memory on 10.0.143.102:45637 (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,499 INFO spark.SparkContext: Created broadcast 80 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,499 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 100 (ShuffledRDD[396] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,499 INFO cluster.YarnScheduler: Adding task set 100.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,500 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 100.0 (TID 78) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,504 INFO storage.BlockManagerInfo: Added broadcast_80_piece0 in memory on algo-1:40231 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,506 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 32 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,511 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 100.0 (TID 78) in 11 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,511 INFO cluster.YarnScheduler: Removed TaskSet 100.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,511 INFO scheduler.DAGScheduler: ResultStage 100 (countByKey at ColumnProfiler.scala:592) finished in 0.015 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,511 INFO scheduler.DAGScheduler: Job 67 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,511 INFO cluster.YarnScheduler: Killing all running tasks in stage 100: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,511 INFO scheduler.DAGScheduler: Job 67 finished: countByKey at ColumnProfiler.scala:592, took 0.049143 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,580 INFO scheduler.DAGScheduler: Registering RDD 401 (collect at AnalysisRunner.scala:326) as input to shuffle 33\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,581 INFO scheduler.DAGScheduler: Got map stage job 68 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,581 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 101 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,581 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,581 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,581 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 101 (MapPartitionsRDD[401] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,584 INFO memory.MemoryStore: Block broadcast_81 stored as values in memory (estimated size 83.4 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,586 INFO memory.MemoryStore: Block broadcast_81_piece0 stored as bytes in memory (estimated size 27.8 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,586 INFO storage.BlockManagerInfo: Added broadcast_81_piece0 in memory on 10.0.143.102:45637 (size: 27.8 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,587 INFO spark.SparkContext: Created broadcast 81 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,587 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 101 (MapPartitionsRDD[401] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,587 INFO cluster.YarnScheduler: Adding task set 101.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,588 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 101.0 (TID 79) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,594 INFO storage.BlockManagerInfo: Added broadcast_81_piece0 in memory on algo-1:40231 (size: 27.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,728 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 101.0 (TID 79) in 140 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,728 INFO cluster.YarnScheduler: Removed TaskSet 101.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,729 INFO scheduler.DAGScheduler: ShuffleMapStage 101 (collect at AnalysisRunner.scala:326) finished in 0.146 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,729 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,729 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,729 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,729 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,751 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,751 INFO scheduler.DAGScheduler: Got job 69 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,751 INFO scheduler.DAGScheduler: Final stage: ResultStage 103 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,751 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 102)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,751 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,751 INFO scheduler.DAGScheduler: Submitting ResultStage 103 (MapPartitionsRDD[404] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,755 INFO memory.MemoryStore: Block broadcast_82 stored as values in memory (estimated size 152.7 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,757 INFO memory.MemoryStore: Block broadcast_82_piece0 stored as bytes in memory (estimated size 43.3 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,757 INFO storage.BlockManagerInfo: Added broadcast_82_piece0 in memory on 10.0.143.102:45637 (size: 43.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,758 INFO spark.SparkContext: Created broadcast 82 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,758 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 103 (MapPartitionsRDD[404] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,758 INFO cluster.YarnScheduler: Adding task set 103.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,759 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 103.0 (TID 80) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,765 INFO storage.BlockManagerInfo: Added broadcast_82_piece0 in memory on algo-1:40231 (size: 43.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,770 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 33 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,827 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 103.0 (TID 80) in 68 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,827 INFO cluster.YarnScheduler: Removed TaskSet 103.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,828 INFO scheduler.DAGScheduler: ResultStage 103 (collect at AnalysisRunner.scala:326) finished in 0.076 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,828 INFO scheduler.DAGScheduler: Job 69 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,829 INFO cluster.YarnScheduler: Killing all running tasks in stage 103: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,829 INFO scheduler.DAGScheduler: Job 69 finished: collect at AnalysisRunner.scala:326, took 0.078634 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,849 INFO codegen.CodeGenerator: Code generated in 17.767636 ms\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,935 INFO codegen.CodeGenerator: Code generated in 20.241933 ms\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,964 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,965 INFO scheduler.DAGScheduler: Got job 70 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,965 INFO scheduler.DAGScheduler: Final stage: ResultStage 104 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,965 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,966 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,966 INFO scheduler.DAGScheduler: Submitting ResultStage 104 (MapPartitionsRDD[414] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,971 INFO memory.MemoryStore: Block broadcast_83 stored as values in memory (estimated size 47.1 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,973 INFO memory.MemoryStore: Block broadcast_83_piece0 stored as bytes in memory (estimated size 19.0 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,973 INFO storage.BlockManagerInfo: Added broadcast_83_piece0 in memory on 10.0.143.102:45637 (size: 19.0 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,974 INFO spark.SparkContext: Created broadcast 83 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,974 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 104 (MapPartitionsRDD[414] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,974 INFO cluster.YarnScheduler: Adding task set 104.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,975 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 104.0 (TID 81) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:51,983 INFO storage.BlockManagerInfo: Added broadcast_83_piece0 in memory on algo-1:40231 (size: 19.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,063 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 104.0 (TID 81) in 88 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,064 INFO cluster.YarnScheduler: Removed TaskSet 104.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,064 INFO scheduler.DAGScheduler: ResultStage 104 (treeReduce at KLLRunner.scala:107) finished in 0.096 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,065 INFO scheduler.DAGScheduler: Job 70 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,066 INFO cluster.YarnScheduler: Killing all running tasks in stage 104: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,066 INFO scheduler.DAGScheduler: Job 70 finished: treeReduce at KLLRunner.scala:107, took 0.101791 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,199 INFO codegen.CodeGenerator: Code generated in 38.904282 ms\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,205 INFO scheduler.DAGScheduler: Registering RDD 419 (collect at AnalysisRunner.scala:326) as input to shuffle 34\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,205 INFO scheduler.DAGScheduler: Got map stage job 71 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,205 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 105 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,205 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,205 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,206 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 105 (MapPartitionsRDD[419] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,209 INFO memory.MemoryStore: Block broadcast_84 stored as values in memory (estimated size 73.5 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,210 INFO memory.MemoryStore: Block broadcast_84_piece0 stored as bytes in memory (estimated size 23.7 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,210 INFO storage.BlockManagerInfo: Added broadcast_84_piece0 in memory on 10.0.143.102:45637 (size: 23.7 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,211 INFO spark.SparkContext: Created broadcast 84 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,211 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 105 (MapPartitionsRDD[419] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,211 INFO cluster.YarnScheduler: Adding task set 105.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,212 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 105.0 (TID 82) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,218 INFO storage.BlockManagerInfo: Added broadcast_84_piece0 in memory on algo-1:40231 (size: 23.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,283 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 105.0 (TID 82) in 71 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,283 INFO cluster.YarnScheduler: Removed TaskSet 105.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,284 INFO scheduler.DAGScheduler: ShuffleMapStage 105 (collect at AnalysisRunner.scala:326) finished in 0.078 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,284 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,284 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,284 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,285 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,406 INFO storage.BlockManagerInfo: Removed broadcast_78_piece0 on 10.0.143.102:45637 in memory (size: 19.2 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,407 INFO storage.BlockManagerInfo: Removed broadcast_78_piece0 on algo-1:40231 in memory (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,413 INFO storage.BlockManagerInfo: Removed broadcast_71_piece0 on 10.0.143.102:45637 in memory (size: 19.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,413 INFO storage.BlockManagerInfo: Removed broadcast_71_piece0 on algo-1:40231 in memory (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,418 INFO storage.BlockManagerInfo: Removed broadcast_83_piece0 on 10.0.143.102:45637 in memory (size: 19.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,419 INFO storage.BlockManagerInfo: Removed broadcast_83_piece0 on algo-1:40231 in memory (size: 19.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,424 INFO storage.BlockManagerInfo: Removed broadcast_81_piece0 on 10.0.143.102:45637 in memory (size: 27.8 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,424 INFO storage.BlockManagerInfo: Removed broadcast_81_piece0 on algo-1:40231 in memory (size: 27.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,426 INFO storage.BlockManagerInfo: Removed broadcast_79_piece0 on 10.0.143.102:45637 in memory (size: 17.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,427 INFO storage.BlockManagerInfo: Removed broadcast_79_piece0 on algo-1:40231 in memory (size: 17.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,428 INFO codegen.CodeGenerator: Code generated in 93.525705 ms\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,429 INFO storage.BlockManagerInfo: Removed broadcast_80_piece0 on 10.0.143.102:45637 in memory (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,430 INFO storage.BlockManagerInfo: Removed broadcast_80_piece0 on algo-1:40231 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,431 INFO storage.BlockManagerInfo: Removed broadcast_84_piece0 on 10.0.143.102:45637 in memory (size: 23.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,432 INFO storage.BlockManagerInfo: Removed broadcast_84_piece0 on algo-1:40231 in memory (size: 23.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,433 INFO storage.BlockManagerInfo: Removed broadcast_82_piece0 on 10.0.143.102:45637 in memory (size: 43.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,434 INFO storage.BlockManagerInfo: Removed broadcast_82_piece0 on algo-1:40231 in memory (size: 43.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,437 INFO storage.BlockManagerInfo: Removed broadcast_73_piece0 on 10.0.143.102:45637 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,438 INFO storage.BlockManagerInfo: Removed broadcast_73_piece0 on algo-1:40231 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,440 INFO storage.BlockManagerInfo: Removed broadcast_74_piece0 on 10.0.143.102:45637 in memory (size: 30.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,441 INFO storage.BlockManagerInfo: Removed broadcast_74_piece0 on algo-1:40231 in memory (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,442 INFO storage.BlockManagerInfo: Removed broadcast_76_piece0 on 10.0.143.102:45637 in memory (size: 19.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,444 INFO storage.BlockManagerInfo: Removed broadcast_76_piece0 on algo-1:40231 in memory (size: 19.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,446 INFO storage.BlockManagerInfo: Removed broadcast_75_piece0 on 10.0.143.102:45637 in memory (size: 49.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,446 INFO storage.BlockManagerInfo: Removed broadcast_75_piece0 on algo-1:40231 in memory (size: 49.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,448 INFO storage.BlockManagerInfo: Removed broadcast_77_piece0 on 10.0.143.102:45637 in memory (size: 26.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,448 INFO storage.BlockManagerInfo: Removed broadcast_77_piece0 on algo-1:40231 in memory (size: 26.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,449 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,449 INFO scheduler.DAGScheduler: Got job 72 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,449 INFO scheduler.DAGScheduler: Final stage: ResultStage 107 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,450 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 106)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,450 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,450 INFO scheduler.DAGScheduler: Submitting ResultStage 107 (MapPartitionsRDD[422] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,452 INFO storage.BlockManagerInfo: Removed broadcast_70_piece0 on 10.0.143.102:45637 in memory (size: 26.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,454 INFO storage.BlockManagerInfo: Removed broadcast_70_piece0 on algo-1:40231 in memory (size: 26.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,455 INFO memory.MemoryStore: Block broadcast_85 stored as values in memory (estimated size 55.1 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,457 INFO memory.MemoryStore: Block broadcast_85_piece0 stored as bytes in memory (estimated size 16.7 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,462 INFO storage.BlockManagerInfo: Added broadcast_85_piece0 in memory on 10.0.143.102:45637 (size: 16.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,462 INFO storage.BlockManagerInfo: Removed broadcast_72_piece0 on 10.0.143.102:45637 in memory (size: 17.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,463 INFO spark.SparkContext: Created broadcast 85 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,463 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 107 (MapPartitionsRDD[422] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,463 INFO cluster.YarnScheduler: Adding task set 107.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,464 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 107.0 (TID 83) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,465 INFO storage.BlockManagerInfo: Removed broadcast_72_piece0 on algo-1:40231 in memory (size: 17.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,475 INFO storage.BlockManagerInfo: Added broadcast_85_piece0 in memory on algo-1:40231 (size: 16.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,478 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 34 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,538 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 107.0 (TID 83) in 74 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,538 INFO cluster.YarnScheduler: Removed TaskSet 107.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,538 INFO scheduler.DAGScheduler: ResultStage 107 (collect at AnalysisRunner.scala:326) finished in 0.084 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,539 INFO scheduler.DAGScheduler: Job 72 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,539 INFO cluster.YarnScheduler: Killing all running tasks in stage 107: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,539 INFO scheduler.DAGScheduler: Job 72 finished: collect at AnalysisRunner.scala:326, took 0.090265 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,581 INFO codegen.CodeGenerator: Code generated in 35.421736 ms\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,645 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,645 INFO scheduler.DAGScheduler: Registering RDD 430 (countByKey at ColumnProfiler.scala:592) as input to shuffle 35\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,646 INFO scheduler.DAGScheduler: Got job 73 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,646 INFO scheduler.DAGScheduler: Final stage: ResultStage 109 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,646 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 108)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,646 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 108)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,646 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 108 (MapPartitionsRDD[430] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,652 INFO memory.MemoryStore: Block broadcast_86 stored as values in memory (estimated size 40.5 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,653 INFO memory.MemoryStore: Block broadcast_86_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,653 INFO storage.BlockManagerInfo: Added broadcast_86_piece0 in memory on 10.0.143.102:45637 (size: 17.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,653 INFO spark.SparkContext: Created broadcast 86 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,654 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 108 (MapPartitionsRDD[430] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,654 INFO cluster.YarnScheduler: Adding task set 108.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,655 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 108.0 (TID 84) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,664 INFO storage.BlockManagerInfo: Added broadcast_86_piece0 in memory on algo-1:40231 (size: 17.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,736 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 108.0 (TID 84) in 81 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,736 INFO cluster.YarnScheduler: Removed TaskSet 108.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,737 INFO scheduler.DAGScheduler: ShuffleMapStage 108 (countByKey at ColumnProfiler.scala:592) finished in 0.090 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,737 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,737 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,737 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 109)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,737 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,737 INFO scheduler.DAGScheduler: Submitting ResultStage 109 (ShuffledRDD[431] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,739 INFO memory.MemoryStore: Block broadcast_87 stored as values in memory (estimated size 5.1 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,740 INFO memory.MemoryStore: Block broadcast_87_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,741 INFO storage.BlockManagerInfo: Added broadcast_87_piece0 in memory on 10.0.143.102:45637 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,745 INFO spark.SparkContext: Created broadcast 87 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,745 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 109 (ShuffledRDD[431] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,746 INFO cluster.YarnScheduler: Adding task set 109.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,747 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 109.0 (TID 85) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,755 INFO storage.BlockManagerInfo: Added broadcast_87_piece0 in memory on algo-1:40231 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,757 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 35 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,763 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 109.0 (TID 85) in 16 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,763 INFO cluster.YarnScheduler: Removed TaskSet 109.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,764 INFO scheduler.DAGScheduler: ResultStage 109 (countByKey at ColumnProfiler.scala:592) finished in 0.026 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,764 INFO scheduler.DAGScheduler: Job 73 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,764 INFO cluster.YarnScheduler: Killing all running tasks in stage 109: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,766 INFO scheduler.DAGScheduler: Job 73 finished: countByKey at ColumnProfiler.scala:592, took 0.121282 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:52,995 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,028 INFO codegen.CodeGenerator: Code generated in 7.240674 ms\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,032 INFO scheduler.DAGScheduler: Registering RDD 436 (count at StatsGenerator.scala:66) as input to shuffle 36\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,032 INFO scheduler.DAGScheduler: Got map stage job 74 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,032 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 110 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,032 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,033 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,033 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 110 (MapPartitionsRDD[436] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,035 INFO memory.MemoryStore: Block broadcast_88 stored as values in memory (estimated size 32.7 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,036 INFO memory.MemoryStore: Block broadcast_88_piece0 stored as bytes in memory (estimated size 13.3 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,037 INFO storage.BlockManagerInfo: Added broadcast_88_piece0 in memory on 10.0.143.102:45637 (size: 13.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,037 INFO spark.SparkContext: Created broadcast 88 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,038 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 110 (MapPartitionsRDD[436] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,038 INFO cluster.YarnScheduler: Adding task set 110.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,039 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 110.0 (TID 86) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,045 INFO storage.BlockManagerInfo: Added broadcast_88_piece0 in memory on algo-1:40231 (size: 13.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,069 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 110.0 (TID 86) in 31 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,069 INFO cluster.YarnScheduler: Removed TaskSet 110.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,071 INFO scheduler.DAGScheduler: ShuffleMapStage 110 (count at StatsGenerator.scala:66) finished in 0.037 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,072 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,072 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,072 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,072 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,083 INFO codegen.CodeGenerator: Code generated in 6.849791 ms\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,092 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,093 INFO scheduler.DAGScheduler: Got job 75 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,093 INFO scheduler.DAGScheduler: Final stage: ResultStage 112 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,093 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 111)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,094 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,094 INFO scheduler.DAGScheduler: Submitting ResultStage 112 (MapPartitionsRDD[439] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,096 INFO memory.MemoryStore: Block broadcast_89 stored as values in memory (estimated size 11.1 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,097 INFO memory.MemoryStore: Block broadcast_89_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,097 INFO storage.BlockManagerInfo: Added broadcast_89_piece0 in memory on 10.0.143.102:45637 (size: 5.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,098 INFO spark.SparkContext: Created broadcast 89 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,098 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 112 (MapPartitionsRDD[439] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,098 INFO cluster.YarnScheduler: Adding task set 112.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,099 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 112.0 (TID 87) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,105 INFO storage.BlockManagerInfo: Added broadcast_89_piece0 in memory on algo-1:40231 (size: 5.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,107 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 36 to 10.0.143.102:45226\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,119 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 112.0 (TID 87) in 20 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,119 INFO cluster.YarnScheduler: Removed TaskSet 112.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,120 INFO scheduler.DAGScheduler: ResultStage 112 (count at StatsGenerator.scala:66) finished in 0.025 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,120 INFO scheduler.DAGScheduler: Job 75 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,121 INFO cluster.YarnScheduler: Killing all running tasks in stage 112: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,121 INFO scheduler.DAGScheduler: Job 75 finished: count at StatsGenerator.scala:66, took 0.028577 s\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,731 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,747 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,793 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,793 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,802 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,819 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,854 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,854 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,869 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,875 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,906 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,906 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,906 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,953 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,954 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-a60f66bd-d0c9-46fa-ba00-201b3465a093\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:53,967 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-36d37d26-6ecf-4279-b611-09dc40ecf8db\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:54,066 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2023-04-20 04:39:54,066 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sagemaker.processing.ProcessingJob at 0x7f52de4ffee0>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=strExecutionRole,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=os.path.join(\n",
    "        strS3BaselineDataUri,\n",
    "        \"train.csv\"\n",
    "    ),\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=strS3BaselineResultsUri,\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86e0588-5679-42cd-91c9-4863a187afe4",
   "metadata": {},
   "source": [
    "* Explore the generated constraints and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bebb3acb-ef34-4aa0-9a0a-f1dbbb08d903",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Files:\n",
      "DJ-SM-IMD/monitor/baselining/results/constraints.json\n",
      " DJ-SM-IMD/monitor/baselining/results/statistics.json\n"
     ]
    }
   ],
   "source": [
    "result = s3_client.list_objects(Bucket=strBucketName, Prefix=strS3BaselineResultsPrefix)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get(\"Contents\")]\n",
    "print(\"Found Files:\")\n",
    "print(\"\\n \".join(report_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4638e5d1-8499-4aae-8ce5-4be09f1a076e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31774/2660869483.py:2: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead.\n",
      "  schema_df = pd.io.json.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>inferred_type</th>\n",
       "      <th>numerical_statistics.common.num_present</th>\n",
       "      <th>numerical_statistics.common.num_missing</th>\n",
       "      <th>numerical_statistics.mean</th>\n",
       "      <th>numerical_statistics.sum</th>\n",
       "      <th>numerical_statistics.std_dev</th>\n",
       "      <th>numerical_statistics.min</th>\n",
       "      <th>numerical_statistics.max</th>\n",
       "      <th>numerical_statistics.distribution.kll.buckets</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.parameters.c</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.parameters.k</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fraud</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>4000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.032750</td>\n",
       "      <td>1.310000e+02</td>\n",
       "      <td>0.177982</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vehicle_claim</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>4000</td>\n",
       "      <td>0</td>\n",
       "      <td>17379.745971</td>\n",
       "      <td>6.951898e+07</td>\n",
       "      <td>10122.664885</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>51051.625749</td>\n",
       "      <td>[{'lower_bound': 1000.0, 'upper_bound': 6005.1...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[15405.795272724228, 12735.637099287029, 3288...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>total_claim_amount</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>4000</td>\n",
       "      <td>0</td>\n",
       "      <td>41248.395971</td>\n",
       "      <td>1.649936e+08</td>\n",
       "      <td>32595.329899</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>588868.568223</td>\n",
       "      <td>[{'lower_bound': 2100.0, 'upper_bound': 60776....</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[19705.79527272423, 30135.637099287032, 25108...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name inferred_type  numerical_statistics.common.num_present  \\\n",
       "0               fraud    Fractional                                     4000   \n",
       "1       vehicle_claim    Fractional                                     4000   \n",
       "2  total_claim_amount    Fractional                                     4000   \n",
       "\n",
       "   numerical_statistics.common.num_missing  numerical_statistics.mean  \\\n",
       "0                                        0                   0.032750   \n",
       "1                                        0               17379.745971   \n",
       "2                                        0               41248.395971   \n",
       "\n",
       "   numerical_statistics.sum  numerical_statistics.std_dev  \\\n",
       "0              1.310000e+02                      0.177982   \n",
       "1              6.951898e+07                  10122.664885   \n",
       "2              1.649936e+08                  32595.329899   \n",
       "\n",
       "   numerical_statistics.min  numerical_statistics.max  \\\n",
       "0                       0.0                  1.000000   \n",
       "1                    1000.0              51051.625749   \n",
       "2                    2100.0             588868.568223   \n",
       "\n",
       "       numerical_statistics.distribution.kll.buckets  \\\n",
       "0  [{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...   \n",
       "1  [{'lower_bound': 1000.0, 'upper_bound': 6005.1...   \n",
       "2  [{'lower_bound': 2100.0, 'upper_bound': 60776....   \n",
       "\n",
       "   numerical_statistics.distribution.kll.sketch.parameters.c  \\\n",
       "0                                               0.64           \n",
       "1                                               0.64           \n",
       "2                                               0.64           \n",
       "\n",
       "   numerical_statistics.distribution.kll.sketch.parameters.k  \\\n",
       "0                                             2048.0           \n",
       "1                                             2048.0           \n",
       "2                                             2048.0           \n",
       "\n",
       "   numerical_statistics.distribution.kll.sketch.data  \n",
       "0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "1  [[15405.795272724228, 12735.637099287029, 3288...  \n",
       "2  [[19705.79527272423, 30135.637099287032, 25108...  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_job = my_default_monitor.latest_baselining_job\n",
    "schema_df = pd.io.json.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "schema_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48fdd71-bf44-42f3-8d58-9dbff3f803b2",
   "metadata": {},
   "source": [
    "### 5.2 Analyze collected data for data quality issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9dfa9c-e999-4b39-857b-ca485d464a8f",
   "metadata": {},
   "source": [
    "* Upload some test scripts to the S3 bucket for pre- and post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "aeeb0696-54e8-4b94-a5c6-9adcd4229b12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/sagemaker-immersion-day/source/monitor\n",
      "s3://sagemaker-us-east-1-419974056037/DJ-SM-IMD/monitor/code/postp/postprocessor.py\n",
      "s3://sagemaker-us-east-1-419974056037/DJ-SM-IMD/monitor/code/prep/preprocessor.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "bucket = boto3.Session().resource(\"s3\").Bucket(strBucketName)\n",
    "strLocalCodePrefix = os.path.join(os.getcwd(), \"source\", \"monitor\")\n",
    "strS3CodePrepKey = os.path.join(\n",
    "    strPrefix,\n",
    "    \"monitor\",\n",
    "    \"code\",\n",
    "    \"prep\",\n",
    "    \"preprocessor.py\"\n",
    ")\n",
    "strS3CodePrepUri = os.path.join(\n",
    "    \"s3://{}\".format(strBucketName),\n",
    "    strS3CodePrepKey\n",
    ")\n",
    "strS3CodePostpKey = os.path.join(\n",
    "    strPrefix,\n",
    "    \"monitor\",\n",
    "    \"code\",\n",
    "    \"postp\",\n",
    "    \"postprocessor.py\"\n",
    ")\n",
    "strS3CodePostpUri = os.path.join(\n",
    "    \"s3://{}\".format(strBucketName),\n",
    "    strS3CodePostpKey\n",
    ")\n",
    "print (strLocalCodePrefix)\n",
    "print (strS3CodePostpUri)\n",
    "print (strS3CodePrepUri)\n",
    "boto3.Session().resource(\"s3\").Bucket(strBucketName).Object(strS3CodePostpKey).upload_file(strLocalCodePrefix + \"/postprocessor.py\")\n",
    "boto3.Session().resource(\"s3\").Bucket(strBucketName).Object(strS3CodePrepKey).upload_file(strLocalCodePrefix + \"/preprocessor.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14af1426-9f89-473b-ae21-b2c91b091165",
   "metadata": {},
   "source": [
    "* Create a schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5866f3a6-639b-451f-beea-f24f3e0e527d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import strftime, gmtime\n",
    "from sagemaker.model_monitor import CronExpressionGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9aa3cb26-414d-424f-9427-121d9593bace",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DEMO-data-drift-monitor-schedule-2023-04-20-04-41-26'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mon_schedule_name = \"DEMO-data-drift-monitor-schedule-\" + strftime(\n",
    "    \"%Y-%m-%d-%H-%M-%S\", gmtime()\n",
    ")\n",
    "\n",
    "strS3ReportPath = os.path.join(\n",
    "    \"s3://{}\".format(strBucketName),\n",
    "    strPrefix,\n",
    "    \"monitor\",\n",
    "    \"report\"\n",
    ")\n",
    "\n",
    "mon_schedule_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ee373e3a-5b19-4260-8776-37e6f36f12a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.model_monitor.model_monitoring:Creating Monitoring Schedule with name: DEMO-data-drift-monitor-schedule-2023-04-20-04-41-26\n"
     ]
    }
   ],
   "source": [
    "my_default_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=mon_schedule_name,\n",
    "    endpoint_input=strEndpointName,\n",
    "    record_preprocessor_script=strS3CodePrepUri,\n",
    "    # post_analytics_processor_script=strS3CodePostpUri,\n",
    "    output_s3_uri=strS3ReportPath,\n",
    "    statistics=my_default_monitor.baseline_statistics(),\n",
    "    constraints=my_default_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a66824-d6c6-4a8d-b63a-8f364896b165",
   "metadata": {},
   "source": [
    "### 5.3 Violations report\n",
    "- https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/model-monitor-interpreting-violations.html\n",
    "\n",
    "    - data_type_check\n",
    "        - 현재 실행의 데이터 유형이 기준 데이터 세트의 데이터 유형과 다를 경우 이 위반에 플래그가 지정됩니다.\n",
    "        - 기준 단계 동안 생성된 제약 조건은 각 열에 대해 추론된 데이터 유형을 제안합니다. 위반으로 플래그가 지정된 경우 임계값을 조정하도록 monitoring_config.datatype_check_threshold 파라미터를 튜닝할 수 있습니다.\n",
    "\n",
    "    - completeness_check\t\n",
    "        - 현재 실행에서 관찰된 완전성(null이 아닌 항목의 %)이 기능별로 지정된 완전성 임계값에 지정된 임계값을 초과하면 이 위반에 플래그가 지정됩니다.\n",
    "        - 기준 단계 동안 생성된 제약 조건은 완전성 값을 제안합니다.\n",
    "\n",
    "    - baseline_drift_check\t\n",
    "        - 현재 데이터 세트와 기준 데이터 세트 간에 계산된 분포 거리가 monitoring_config.comparison_threshold에 지정된 임계값보다 크면 이 위반에 플래그가 지정됩니다.\n",
    "    \n",
    "    - missing_column_check\t\n",
    "        - 현재 데이터 세트의 열 수가 기준 데이터 세트의 개수보다 작으면 이 위반에 플래그가 지정됩니다.\n",
    "\n",
    "    - extra_column_check\t\n",
    "        - 현재 데이터 세트의 열 수가 기준의 개수보다 많으면 이 위반에 플래그가 지정됩니다.\n",
    "\n",
    "    - categorical_values_check\t\n",
    "        - 현재 데이터 세트의 알 수 없는 값이 기준 데이터 세트보다 더 많으면 이 위반에 플래그가 지정됩니다. 이 값은 monitoring_config.domain_content_threshold의 임계값에 의해 결정됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f68f8865-7759-4640-af9e-9b4cab3cf083",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31774/4018518515.py:3: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead.\n",
      "  constraints_df = pd.io.json.json_normalize(violations.body_dict[\"violations\"])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "violations = my_default_monitor.latest_monitoring_constraint_violations()\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "constraints_df = pd.io.json.json_normalize(violations.body_dict[\"violations\"])\n",
    "constraints_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
